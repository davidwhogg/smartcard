% this file is part of the Smart Card project.

% to-do
% -----
% - finish zeroth draft
% - get all citations in
% - get all figures in
% - search document for all HOGG CITE todo etc.
% - make sure budget and timeline are consistent.

\documentclass[letterpaper,12pt,preprint]{hack_aastex}

\usepackage{amsmath}
\usepackage{color}
\usepackage[pagebackref=false]{hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.25}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\newcommand{\hurl}[1]{{\scriptsize\url{#1}}}

\usepackage{epsfig}
\usepackage{graphicx}
\newcommand{\sectionname}{Section}
\setlength{\headheight}{2ex}
\setlength{\headsep}{2ex}
\input{hogg_nasa}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\bvec}[1]{{\ensuremath{{\boldsymbol{#1}}}}}
\pagestyle{myheadings}
\markright{\textsf{\footnotesize Hogg \& Foreman-Mackey / %
                   End-to-end probabilistic modeling of Kepler data}}

\usepackage{listings}
\lstset{%
    language=Python,
    basicstyle=\scriptsize\ttfamily,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    breaklines=false,
    breakatwhitespace=true,
    identifierstyle=\ttfamily,
    keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
    commentstyle=\color[rgb]{0.4,0.4,0.4},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\newcommand{\documentname}{\textsl{Proposal}}

\begin{document}

\section{Introduction?}

By discovering thousands of transiting exoplanets, the NASA \Kepler\ Mission has
transformed the study of extra-Solar planets (exoplanets).
In addition to system discoveries, \Kepler\ has enabled
extremely precise measurements,
in which we can see planet--planet interactions (CITE), direct emission from
hot planets (CITE), and even relativistic effects (CITE).

Most importantly from the perspective of this \documentname,
\Kepler\ data have revealed large populations of very small planets,
Earth radius and smaller (CITE).
We now know that most or all of these small planets are almost certainly rocky
(CITE).
Some of these rocky planets even orbit in the putative ``habitable zones'' of
their parent stars, as determined by expected average insolation
(we don't yet know anything about small-planet atmospheres).
In \figurename~\ref{fig:planet-properties} we show the distribution of known
planets in radius ratio (planet-to-star) and orbital period, with the location
of Earth shown.
This is promising for the study of Earth analogs!

\ssfigure{figures/planet_properties.pdf}{0.60}{%
The properties of confirmed and candidate exoplanets from \Kepler.
Data from NASA Exoplanet Archive.
The candidates near the location of Earth (labeled by grey lines) are orbiting
lower-mass (cooler, fainter) stars and are therefore not true Earth analogs,
in terms of size, period, and temperature (or insolation).
\label{fig:planet-properties}}

That said, no \emph{true} Earth analog is yet known.
The known rocky, plausibly habitable exoplanets have all been found around
stars lower mass (and cooler) than the Sun.
They are easier to find around low-mass stars, of course,
because the radius ratio is more favorable, and the habitable zone orbits
correspond to shorter periods.
It also appears to be the case that the abundance of rocky planets around
low-mass stars is higher than that around G-type stars (CITE).

That said, the \Kepler\ data \emph{are} good enough to detect true Earth
analogs.
There are thousands of G-type stars in the \Kepler\ dataset
that are bright enough (in a photon-counting sense) that an Earth-sized
transiting planet on a year-ish period orbit ought to be detectable at good
confidence.
In a project underway now, we are checking whether it is likely---given transit
probabilities under random system inclinations and extrapolations of the
exoplanet population to Earth properties---that the dataset includes a
transiting Earth analog among these thousands of targets.
Some studies say yes (CITE Petigura).
Either way, it would be a crime against all of astronomy if the \Kepler\ data
set is not searched for Earth analogs with tools capable of saturating the
photon-statistics bounds on discovery and inference.

Here we propose to build these tools.
We propose four tools, each capable of increasing the sensitivity and
statistical power of the \Kepler\ mission.
The tools will enable the most sensitive search for Earth analogs ever
conducted.
They will also have wide applicability to other missions, projects, and
objectives.
This is a project that the NASA ADAP program was born to support.

Why is finding an Earth analog so hard?
Why not just loop over periods and phases and start chi-squared fitting?
Fundamentally, the reason is that the \Kepler\ Mission is so very very good:
Never before has photometry been performed at this level of precision (part
in $10^{5}$ or better in terms of raw photon precision);
sure enough, at this unprecedented precision, unprecedented astronomical
issues arise.

[todo] There are photons to do it, but various issues:  Calibration, telescope variations, stellar stochastic variability, lack of understanding of the full system.

[todo] We should say something about how \Kepler\ operates:  Fixed pointing for three-month periods.  Insensitivity to flat-field and PSF.  But imperfect.

\section{A data-driven model of the \Kepler\ pixels}

Despite the impressive precision, \Kepler's photometry is plagued by
substantial ``systematics'' due to instrumental effects (pointing shifts,
temperature variations, \etc) and real astrophysical signals (stellar
variability, transiting exoplanets, \etc).
It is of significant scientific interest to separate these two types of
signals and much progress has been made towards removing instrumental
systematics while robustly retaining the astrophysical effects (CITE PDC, ARC,
\etc).
These algorithms are all based on a fundamental \emph{causal} argument: the
signals that are common across nearby targets must be due to instrumental
variations because there can be no causal connection between the astrophysical
objects.
The idea is simple but a lot of the work that goes into implementing these
models involves combating over-fitting.

We propose to implement a method based on the same argument that models the
instrumental effects \emph{at the pixel level} instead of in the photometry.
This method makes a prediction for the variability caused by the instrument in
a specific pixel at a specific time by using the pixel time series of similar
nearby (but causally disconnected) targets at different times.
This prediction can then be used to remove or model the systematics in various
ways but our current thinking is that this model is best used TODO photometry
of prediction.

We'll model the flux due to systematic variations in pixel $k$ of target
$n$ at time $t$ as
\begin{eqnarray}
f_{nk}(t) &=& \bvec{c}_{nk}^\mathrm{T}\cdot\bvec{f}_{\sim n}(t)
              + \epsilon_{nk}(t)
\end{eqnarray}
where $\bvec{f}_{\sim n}$ is the vector of some $K$ pixels around nearby
targets (not including $n$), $\bvec{c}_{nk}$ is a vector of linear weights,
and $\epsilon_{nk}$ represents the stochastic pixel noise.
There are many choices that must be made to evaluate this model but the main
ones are: (a) the number of pixels $K$ should be used in $\bvec{f}_{\sim n}$,
and (b) how the weights $\bvec{c}_{nk}$ are chosen.
Motivated by the standard techniques in the machine learning literature, we
advocate for a very large value of $K$ and use other techniques to regularize
and avoid over-fitting.
In particular, we choose to fit the coefficients $\bvec{c}$ using the light
curve in this pixel but at \emph{different times} $t^\prime$ where
$|t-t^\prime| > \Delta$.

\ssfigure{figures/kepler-20-plm.pdf}{0.60}{%
A comparison of photometric methods applied to quarter 9 of target Kepler-20,
a variable exoplanet host with 5 known transiting exoplanets.
This star is known to exhibit significant stellar variability and we chose it
as an example because the presearch data conditioning algorithm clearly
over-fits the signals.
With sensible choices for the hyperparameters of our data-driven model, the
resulting light curve retains more of the interesting astrophysical signals
and does not affect the transit depths.
\emph{Top:} the simple aperture photometry (black points) with the data-driven
prediction for the instrumental effects.
\emph{Middle:} the data-driven photometric light curve.
\emph{Bottom:} the presearch data conditioned light curve.
\label{fig:plm}}

[todo] What, precisely, do we propose?

\section{Optimized aperture photometry}

The astronomical community tends to think in terms of stellar flux estimators
(or photometric estimators) that are weighted linear sums of pixels.
If you have an image of an isolated star and you know it's position in the
image, the point-spread function, the approximate brightness of the star, and
all the parameters of the pixel-level noise model, it is possible to obtain a
photometric estimate of the star of this form (weighted linear sum of pixels)
that is optimized for signal-to-noise.
The estimator is a kind of ``matched filter''.
This result has been known for a long time (CITE THINGS).

Below (when we talk about \kpsf) we are going to consider more radical
photometric estimators.
But even within the restriction to estimators based on weighted linear sums,
there are advances to be made.
In the case of \Kepler\ the opportunities come from the fact that the data
are taken in an extremely uniform, homogeneous mode, but at the same time not
\emph{perfectly} homogeneous.
That is, the pointing of \Kepler\ is stable only at the hundredth-of-a-pixel
level (or maybe thousandth; we don't precisely know), and it undergoes
temperature and focus variations as it adjusts for attitude changes required
for data downlink operations.

If we think of the ``jitter'' or small unknown displacements of the angular
position of the satellite and the optics in focus or temperature changes as a
kind of ``noise'', then even though the individual pixel read-outs are
independent, the variance in the pixel space obtains non-trivial covariances:
The pointing and PSF variance maps onto a correlated pixel variance.

Ideally, we would model these using knowledge of the \Kepler\ PSF and
pointing jitter.
Unfortunately, neither of these things is precisely known at present (but see
our \kpsf\ proposal below).
They aren't known, but there are a lot of data!
It turns out that we can determine the ``s/c jitter noise'' (really a pointing,
focus, and temperature noise) empirically;
we can measure the empirical mean and variance of the pixels in the patch of
imaging centered on each relevant star.

If you think of a read-out pixel patch from exposure $n$ taken at time
$t_n$ as being a little data ``vector'' $d_n$ (19 pixels or a 19-vector
in the example shown in \figurename~\ref{fig:owl}), then we can obtain an
empirical mean $\mu$ (19-vector) and an empirical covariance $C$ ($19\times 19$
symmetric, positive definite matrix).
If the mean and covariance are well estimated---that is, if they
represent the true mean jittered PSF and the true covariance of the
pixel values around that mean---and if the covariance is dominated by
telescope and detector noise sources---that is, not intrinsic
variability of the star---then these estimated quantities $\mu$ and
$C$ can be used to construct optimal weights for linear photometric
estimators on the data.
These estimators are truly novel:
They are signal-to-noise-optimized estimators in the presence of noise coming
from spacecraft positional jitter and other effects that correlate pixel noise.
However, they are also simple generalizations of the ``matched filter''
estimators used to perform optimal photometry and optimal spectroscopic
extraction (CITE).
We show an example in \figurename~\ref{fig:owl}.
We propose here to build a software package called ``the \OWL'' that constructs
these estimators.
We also propose to explore the decision space that we allude to below.

%% HOGG put fig:owl here
%% Include the usual figure and the ``d'' figure
%% Long caption

It turns out that there are many photometric estimators constructable as
optimizations of some estimate of signal-to-noise; for this reason we call 
the outputs of the \OWL\ ``optimized'' rather than ``optimal'' estimators;
no estimator is optimal for all purposes.
One class of decisions is related to how we estimate the empirical mean $\mu$
and covariance $C$:
Do we clip outliers?
Do we consider variations on all time-scales?
Do we somehow include or exclude intrinsic stellar variability?

Another class of decisions is related to regularization or control of the
photometric aperture.
In the results shown in \figurename~\ref{fig:owl} we permitted an arbitrarly
shaped and weighted aperture in pixel space.
It would be possible to restrict the aperture to have a round shape, or a
top-hat shape, or a Gaussian, or whatever.
For each such choice---and each method of estimating $\mu$ and $C$---there is
an optimal aperture for performing photometry.
All these choices need to be explored.

The output of the \OWL\ is a new photometric aperture, which can be used to
create photometry that is perfectly analogous to the existing \Kepler\
photometry.
The existing photometry is obtained through an optimized aperture, but that
aperture is optimized using a theoretical point-spread function, ideas about
field crowding, and assumptions of minimal s/c jitter.
The \OWL\ produces photometry that is either higher in signal-to-noise or else
detrended from s/c movements, or (in excellent cases) both.
That is, the \OWL\ photometry, built as it is with a data-driven optimization,
is expected to be better than any existing \Kepler\ product.

One key idea of this \documentname\ is that the \OWL\ plays very well the \PLM.
That is, as the \PLM\ does a better and better job of calibrating the
instrument (and the \PLM\ effectively calibrates out some parts of the s/c
jitter noise), the empirical covariance seen by the \OWL\ will either be
smaller in amplitude or else informed by the \PLM\ model directly....HOGG

...to the 

[todo] decisions about how to estimate mean and covariance.  Some ideas.

[todo] decisions about how to constrain or regularize the weights.  Some ideas.

[todo] include a figure that shows success.

[todo] what, precisely, do we propose?  Finish \OWL\ software, explore choice space, produce useful \Kepler\ alternative photometry, release code and documentation, write a paper (or maybe two).

\section{Flexible, non-parametric modeling of light curve variations}

[todo] Why are stochastic stellar variations a killer?  In what sense do they make naive $\chi^2$ fitting bad?

[todo] What is a non-parametric model?  What is a Gaussian Process?

[todo] What does the GP likelihood look like?

[todo] What are the hyperparameters and how can we set them; what do they do?

[todo] How does this relate to search and what searches are we going to do?

[todo] what, precisely, do we propose?

\ssfigure{figures/kic-10593626-synth.pdf}{0.60}{%
\Kepler\ short-cadence data for KIC 10593626 with a synthetic injected transit
(points) and 50 posterior samples (lines) from an MCMC analysis with nine
parameters (7 physical and 2 hyperparameters for the noise model).
\label{fig:george}}

\section{The \Kepler\ focal plane}

[todo] Reminder of the importance of a LF, both for frequentists and Bayesians.

[todo] A full probability for the data given parameters would look like a causal model, with PSF, flat-field and so on.  Refer out to demos in the white paper.

[todo] What baby steps have we taken so far and what kinds of decisions will there be?

[todo] How does this interact with the \PLM\ and the \OWL?  Is it in concert or conflict?  Modify summary to be consistent.

[todo] what, precisely, do we propose?

\section{Management plan and schedule}

[todo] In the first year, we do OWL and George.  Hogg leads \OWL, DFM leads \George.  DFM starts search as a full-scale test of George and etc.

[todo] In the second year, we do PLM, let by postdoc.  Finish search.

[todo] In the third year we do kpsf, led by postdoc.  and re-run search.

\clearpage
\begin{thebibliography}{}\raggedright%

\bibitem[Petigura \etal(2013)]{petigura}
Petigura, E.~A., Howard, A.~W., \& Marcy, G.~W.\ 2013,
Proceedings of the National Academy of Science, 110, 19273

\end{thebibliography}

\end{document}
