% this file is part of the Smart Card project.

% to-do
% -----
% - finish zeroth draft
% - get all citations in
% - get all figures in
% - search document for all HOGG CITE todo TODO etc.
% - make sure budget and timeline are consistent.

\documentclass[letterpaper,12pt,preprint]{hack_aastex}

\usepackage{amsmath}
\usepackage{color}
\usepackage[pagebackref=false]{hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.25}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\newcommand{\hurl}[1]{{\scriptsize\url{#1}}}

\usepackage{epsfig}
\usepackage{graphicx}
\newcommand{\sectionname}{Section}
\setlength{\headheight}{2ex}
\setlength{\headsep}{2ex}
\input{hogg_nasa}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\bvec}[1]{{\ensuremath{{\boldsymbol{#1}}}}}
\pagestyle{myheadings}
\markright{\textsf{\footnotesize Hogg \& Foreman-Mackey / %
                   End-to-end probabilistic modeling of Kepler data}}

\usepackage{listings}
\lstset{%
    language=Python,
    basicstyle=\scriptsize\ttfamily,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    breaklines=false,
    breakatwhitespace=true,
    identifierstyle=\ttfamily,
    keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
    commentstyle=\color[rgb]{0.4,0.4,0.4},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\newcommand{\documentname}{\textsl{Proposal}}

\begin{document}

\section{Introduction?}

By discovering thousands of transiting exoplanets, the NASA \Kepler\ Mission has
transformed the study of extra-Solar planets (exoplanets).
In addition to system discoveries, \Kepler\ has enabled
extremely precise measurements,
in which we can see planet--planet interactions (CITE), direct emission from
hot planets (CITE), and even relativistic effects (CITE).

Most importantly from the perspective of this \documentname,
\Kepler\ data have revealed large populations of very small planets,
Earth radius and smaller (CITE).
We now know that most or all of these small planets are almost certainly rocky
(CITE).
Some of these rocky planets even orbit in the putative ``habitable zones'' of
their parent stars, as determined by expected average insolation
(we don't yet know anything about small-planet atmospheres).
In \figurename~\ref{fig:planet-properties} we show the distribution of known
planets in radius ratio (planet-to-star) and orbital period, with the location
of Earth shown.
This is promising for the study of Earth analogs!

\ssfigure{figures/planet_properties.pdf}{0.60}{%
The properties of confirmed and candidate exoplanets from \Kepler.
Data from NASA Exoplanet Archive.
The candidates near the location of Earth (labeled by grey lines) are orbiting
lower-mass (cooler, fainter) stars and are therefore not true Earth analogs,
in terms of size, period, and temperature (or insolation).
\label{fig:planet-properties}}

That said, no \emph{true} Earth analog is yet known.
The known rocky, plausibly habitable exoplanets have all been found around
stars lower mass (and cooler) than the Sun.
They are easier to find around low-mass stars, of course,
because the radius ratio is more favorable, and the habitable zone orbits
correspond to shorter periods.
It also appears to be the case that the abundance of rocky planets around
low-mass stars is higher than that around G-type stars (CITE).

That said, the \Kepler\ data \emph{are} good enough to detect true Earth
analogs.
There are thousands of G-type stars in the \Kepler\ dataset
that are bright enough (in a photon-counting sense) that an Earth-sized
transiting planet on a year-ish period orbit ought to be detectable at good
confidence.
In a project underway now, we are checking whether it is likely---given transit
probabilities under random system inclinations and extrapolations of the
exoplanet population to Earth properties---that the dataset includes a
transiting Earth analog among these thousands of targets.
Some studies say yes (CITE Petigura).
Either way, it would be a crime against all of astronomy if the \Kepler\ data
set is not searched for Earth analogs with tools capable of saturating the
photon-statistics bounds on discovery and inference.

Here we propose to build these tools.
We propose four tools, each capable of increasing the sensitivity and
statistical power of the \Kepler\ mission.
The tools will enable the most sensitive search for Earth analogs ever
conducted.
They will also have wide applicability to other missions, projects, and
objectives.
This is a project that the NASA ADAP program was born to support.

Why is finding an Earth analog so hard?
Why not just loop over periods and phases and start chi-squared fitting?
Fundamentally, the reason is that the \Kepler\ Mission is so very very good:
Never before has photometry been performed at this level of precision (part
in $10^{5}$ or better in terms of raw photon precision);
sure enough, at this unprecedented precision, unprecedented astronomical
issues arise.

For example, even though the satellite points to much better than a hundredth
of a pixel, the tiny variations in pointing cause tiny fractional differences
in the amounts of light hitting neighboring pixels in the vicinity of any
\Kepler\ target.
This variation projected onto any unmodeled flat-field residual or sub-pixel
flat-field variation leads to variations in total counts in the objects.
Similar things happen for temperature-induced optical distortion variations,
and focus changes.
These problems are currently not well modeled by the \Kepler\ official pipelines
and present challenges for every \Kepler\ user working at high precision.

In addition, it turns out that at this level of precision, the great majority
of stars---even the great majority of Sun-like stars---are photometrically
variable in stochastic ways.
That is, there is ``intrinsic'' noise coming from the stars themselves,
over and above photon shot noise.

For context for what follows:
The \Kepler\ mission operates in an extremely simple mode that is designed to
make it insensitive to residual calibration issues in its optics and detector.
For example, it spends three-month periods with its pointing locked down to
much better than pixel precision.
This permits precise photometry without full understanding of the flat-field
or PSF.
Exposures are typically 30~min;
for each target star, not all of the pixels are downloaded every exposure.
Instead, to preserve bandwidth, only a small pixel patch is downloaded for each
star.
Finally, within the downloaded pixel patch, the official \Kepler\ photometry
is based on a straight sum of just a few pixels more-or-less centered on the
star.
As we hope to explain in this \documentname, there is low-hanging fruit to be
picked to make \Kepler's light curves more precise.

Everything in this \documentname\ is about \Kepler, but of course everything we
propose will be released open-source (as is everything done in our group).
That means that everything we propose here that is useful for \Kepler\ will
also be useful for the \textsl{K2} extended mission, that was just (2014 May 15)
approved for funding in the NASA Response to the 2014 Senior Review.

\section{A data-driven model of the \Kepler\ pixels}

Despite the impressive precision, \Kepler's photometry is plagued by
substantial ``systematics'' due to instrumental effects (pointing shifts,
temperature variations, \etc) and real astrophysical signals (stellar
variability, transiting exoplanets, \etc).
It is of significant scientific interest to separate these two types of
signals and much progress has been made towards removing instrumental
systematics while robustly retaining the astrophysical effects (CITE PDC, ARC,
\etc).
These algorithms are all based on a fundamental \emph{causal} argument: the
signals that are common across nearby targets must be due to instrumental
variations because there can be no causal connection between the astrophysical
objects.
The idea is simple but a lot of the work that goes into implementing these
models involves combating over-fitting.

We propose to implement a method based on the same argument that models the
instrumental effects \emph{at the pixel level} instead of in the photometry.
This method makes a prediction for the variability caused by the instrument in
a specific pixel at a specific time by using the pixel time series of similar
nearby (but causally disconnected) targets at different times.
This prediction can then be used to remove or model the systematics in various
ways but our current thinking is that this model is best used TODO photometry
of prediction.

We'll model the flux due to systematic variations in pixel $k$ of target
$n$ at time $t$ as
\begin{eqnarray}
f_{nk}(t) &=& \bvec{c}_{nk}^\mathrm{T}\cdot\bvec{f}_{\sim n}(t)
              + \epsilon_{nk}(t)
\end{eqnarray}
where $\bvec{f}_{\sim n}$ is the vector of some $K$ pixels around nearby
targets (not including $n$), $\bvec{c}_{nk}$ is a vector of linear weights,
and $\epsilon_{nk}$ represents the stochastic pixel noise.
There are many choices that must be made to evaluate this model but the main
ones are: (a) the number of pixels $K$ should be used in $\bvec{f}_{\sim n}$,
and (b) how the weights $\bvec{c}_{nk}$ are chosen.

In principle, as $K$ gets large, the model will tend to over-fit the data.
Motivated by the standard techniques in the machine learning literature, we
advocate for a very large value of $K$ but to use other techniques to regularize
and avoid over-fitting.
In particular, we choose to fit the coefficients $\bvec{c}$ using the light
curve in this pixel but at \emph{different times} $t^\prime$ where
$|t-t^\prime| > \Delta$.
That is, we use a ``train-and-test'' framework to control model freedom.
We also use some Gaussian regularization that draws weakly controlled parameters
towards zero.
\figurename~\ref{fig:plm} shows an example, in comparison with standard \Kepler
data products.

\ssfigure{figures/kepler-20-plm.pdf}{0.60}{%
A comparison of photometric methods applied to quarter 9 of target Kepler-20,
a variable exoplanet host with 5 known transiting exoplanets.
This star is known to exhibit significant stellar variability and we chose it
as an example because the presearch data conditioning algorithm clearly
over-fits the signals.
With sensible choices for the hyperparameters of our data-driven model, the
resulting light curve retains more of the interesting astrophysical signals
and does not affect the transit depths.
\emph{Top:} The \Kepler\ simple aperture photometry (black points) with the data-driven
prediction for the instrumental effects.
\emph{Middle:} Our data-driven photometric light curve, which removes instrumental effects but preserves stellar variability and transit depths.
\emph{Bottom:} The \Kepler\ presearch data conditioning light curve, which over-fits the data, removing real variability.
\label{fig:plm}}

The \PLM\ produces a prediction for every pixel under the assumption
(effectively) of a non-variable sky.
How do we plan to use this prediction?
There are several options, including subtracting it away or dividing it out.
Our plan is to photometer the data according to the \Kepler\ SAP prescription,
and then also photometer the \PLM\ prediction of the data, and present the
ratio of these as the \PLM\ photometry.
In detail, this is what \figurename~\ref{fig:plm} shows.
(We have shown a variable star in \figurename~\ref{fig:plm}, to emphasize that
the \PLM\ preserves intrinsic variability and delivers very different results
from the official \Kepler\ ``detrending'' PDC pipeline.)

Here we propose to build the \PLM\ and work through the choices of
train-and-test subsampling, regularization, and $K$ to find high-quality
options for the \Kepler\ data.
We propose to run it on a significant fraction of the
\Kepler\ data (at least all the Sun-like stars).
We also propose to release the code in a very useable form (for example, ``input
a \Kepler\ target and get back the \PLM\ photometry'') so that it can be used
by anyone in the community, including the \textsl{K2} team.
We propose to write up at least one paper in the refereed literature describing
the \PLM.
In what follows, we will propose further to use the \PLM\ model and photometry
as inputs to our photometric estimator codes and variability modeling codes.

\section{Optimized aperture photometry}

The astronomical community tends to think in terms of stellar flux estimators
(or photometric estimators) that are weighted linear sums of pixels.
If you have an image of an isolated star and you know it's position in the
image, the point-spread function, the approximate brightness of the star, and
all the parameters of the pixel-level noise model, it is possible to obtain a
photometric estimate of the star of this form (weighted linear sum of pixels)
that is optimized for signal-to-noise.
The estimator is a kind of ``matched filter''.
This result has been known for a long time (CITE THINGS).

Below (when we talk about \kpsf) we are going to consider more radical
photometric estimators.
But even within the restriction to estimators based on weighted linear sums,
there are advances to be made.
In the case of \Kepler\ the opportunities come from the fact that the data
are taken in an extremely uniform, homogeneous mode, but at the same time not
\emph{perfectly} homogeneous.
That is, the pointing of \Kepler\ is stable only at the hundredth-of-a-pixel
level (or maybe thousandth; we don't precisely know), and it undergoes
temperature and focus variations as it adjusts for attitude changes required
for data downlink operations.

If we think of the ``jitter'' or small unknown displacements of the angular
position of the satellite and the optics in focus or temperature changes as a
kind of ``noise'', then even though the individual pixel read-outs are
independent, the variance in the pixel space obtains non-trivial covariances:
The pointing and PSF variance maps onto a correlated pixel variance.

Ideally, we would model these using knowledge of the \Kepler\ PSF and
pointing jitter.
Unfortunately, neither of these things is precisely known at present (but see
our \kpsf\ proposal below).
They aren't known, but there are a lot of data!
It turns out that we can determine the ``s/c jitter noise'' (really a pointing,
focus, and temperature noise) empirically;
we can measure the empirical mean and variance of the pixels in the patch of
imaging centered on each relevant star.

If you think of a read-out pixel patch from exposure $n$ taken at time
$t_n$ as being a little data ``vector'' $d_n$ (19 pixels or a 19-vector
in the example shown in \figurename~\ref{fig:owl}), then we can obtain an
empirical mean $\mu$ (19-vector) and an empirical covariance $C$ ($19\times 19$
symmetric, positive definite matrix).
If the mean and covariance are well estimated---that is, if they
represent the true mean jittered PSF and the true covariance of the
pixel values around that mean---and if the covariance is dominated by
telescope and detector noise sources---that is, not intrinsic
variability of the star---then these estimated quantities $\mu$ and
$C$ can be used to construct optimal weights for linear photometric
estimators on the data.
These estimators are truly novel:
They are signal-to-noise-optimized estimators in the presence of noise coming
from spacecraft positional jitter and other effects that correlate pixel noise.
However, they are also simple generalizations of the ``matched filter''
estimators used to perform optimal photometry and optimal spectroscopic
extraction (CITE).
We show an example in \figurename~\ref{fig:owl}.
We propose here to build a software package called ``the \OWL'' that constructs
these estimators.
We also propose to explore the decision space that we allude to below.

\ssfiguretwo{figures/kic_03335426_05_images_dopw.png}%
            {figures/kic_03335426_05_diff_photometry.png}{0.60}{%
An example of the \OWL\ photometry acting on one quarter of \Kepler\ data
on one target star.
In the top panel, one of the optimized soft photometric apertures is shown,
in comparison with the standard \Kepler\ SAP aperture.
In the bottom panel the photometric lightcurves are shown.
The curves marked ``DOWL'', ``DOPW'', and ``DTSA'' are different versions of the
\OWL\ method.
\OWL\ removes the trends completely from the data,
revealing a low-level rotation-period variability (presumably from star spots).
The removal of the trends comes at some cost in pixel-to-pixel variability,
because the soft aperture created by the \OWL\ is forced to be orthogonal to
substantial positional jitter noise.
These results are very preliminary, but they demonstrate feasibility.
\label{fig:owl}}

It turns out that there are many photometric estimators constructable as
optimizations of some estimate of signal-to-noise; for this reason we call
the outputs of the \OWL\ ``optimized'' rather than ``optimal'' estimators;
no estimator is optimal for all purposes.
One class of decisions is related to how we estimate the empirical mean $\mu$
and covariance $C$:
Do we clip outliers?
Do we consider variations on all time-scales?
Do we somehow include or exclude intrinsic stellar variability?

Another class of decisions is related to regularization or control of the
photometric aperture.
In the results shown in \figurename~\ref{fig:owl} we permitted an arbitrarly
shaped and weighted aperture in pixel space.
It would be possible to restrict the aperture to have a round shape, or a
top-hat shape, or a Gaussian, or whatever.
For each such choice---and each method of estimating $\mu$ and $C$---there is
an optimal aperture for performing photometry.
All these choices need to be explored.

The output of the \OWL\ is a new photometric aperture, which can be used to
create photometry that is perfectly analogous to the existing \Kepler\
photometry.
The existing photometry is obtained through an optimized aperture, but that
aperture is optimized using a theoretical point-spread function, ideas about
field crowding, and assumptions of minimal s/c jitter.
The \OWL\ produces photometry that is either higher in signal-to-noise or else
detrended from s/c movements, or (in excellent cases) both.
That is, the \OWL\ photometry, built as it is with a data-driven optimization,
is expected to be better than any existing \Kepler\ product,
at least for isolated stars that are low in intrinsic variability
(more on this below).

One key idea of this \documentname\ is that the \OWL\ plays very well the \PLM.
That is, as the \PLM\ does a better and better job of calibrating the
instrument (and the \PLM\ effectively calibrates out some parts of the s/c
jitter noise), the empirical covariance seen by the \OWL\ will either be
smaller in amplitude or else informed by the \PLM\ model directly;
indeed the \PLM\ is effectively a model of all the s/c jitter contributions to
the empirical noise covariance.

We propose to fully develop the \OWL\ software and explore the choices available
in estimation of $\mu$ and $C$ and also in aperture regularization or
constraint.
We propose to run the tuned \OWL\ on a large number of \Kepler\ targets
(certainly including all the Sun-like stars).
We also propose to release all the code in a simple-to-use form so that it
can be adopted by any \Kepler\ user and also the \textsl{K2} team if they
want it.
We propose to write up the \OWL\ in at least one paper in the scientific
literature.

\section{Flexible, non-parametric modeling of light curve variations}

[todo] Why are stochastic stellar variations a killer?  In what sense do they make naive $\chi^2$ fitting bad?

[todo] What is a non-parametric model?  What is a Gaussian Process?

[todo] What does the GP likelihood look like?

[todo] What are the hyperparameters and how can we set them; what do they do?

[todo] How does this relate to search and what searches are we going to do?

[todo] what, precisely, do we propose?

\ssfigure{figures/kic-10593626-synth.pdf}{0.60}{%
\Kepler\ short-cadence data for KIC 10593626 with a synthetic injected transit
(points) and 50 posterior samples (lines) from an MCMC analysis with nine
parameters (7 physical and 2 hyperparameters for the noise model).
\label{fig:george}}

\section{The \Kepler\ focal plane}

In a perfect world where we could infer a full physical, time-dependent model
of \Kepler's focal plane, the \PLM\ and \OWL\ procedures would be obviated.
In practice, this is a very difficult problem and data-driven models might
continue to be the best method for measuring ultra-precise relative photometry
for isolated stars.
That being said, there are some places where a robust model of the focal
plane---including the flat field and the point spread function, for
example---is necessary.
There are, for example, some blended sources in the \Kepler\ field-of-view and
de-blending the light curves requires a full likelihood model (\citealt{psf}).
Studies of full-pipeline recovery reliability (\citealt{inject}) involve
injecting transit signals at the pixel level.
This operation is only as reliable as our model of the focal plane.
If the next-generation \Kepler\ mission is approved, the pointing of the
telescope will be much less stable than during the original mission.
In this case, a model of the focal plane will be useful for measuring
photometry of the stars as they drift.

In order to improve these procedures and (possibly) generate more precise
light curves, we propose to build a full probabilistic generative model for
the flat field and point spread function of the \Kepler\ detector.


[todo] Reminder of the importance of a LF, both for frequentists and Bayesians.

[todo] A full probability for the data given parameters would look like a causal model, with PSF, flat-field and so on.  Refer out to demos in the white paper.

[todo] What baby steps have we taken so far and what kinds of decisions will there be?

[todo] How does this interact with the \PLM\ and the \OWL?  Is it in concert or conflict?  Modify summary to be consistent.

[todo] what, precisely, do we propose?

\section{Management plan and schedule}

[todo] In the first year, we do OWL and George.  Hogg leads \OWL, DFM leads \George.  DFM starts search as a full-scale test of George and etc.

[todo] In the second year, we do PLM, let by postdoc.  Finish search.

[todo] In the third year we do kpsf, led by postdoc.  and re-run search.

All code will be released under the MIT open-source license.
This is a simple license that permits reuse, modification, and re-release of
the code.
We also keep all code available on the Web and licensed at every stage of
the project, so in fact the community can intervene, comment, or build on our
work at any stage in the development process.

\clearpage
\begin{thebibliography}{}\raggedright%

\bibitem[Christiansen \etal(2013)]{inject}
Christiansen, J.~L., Clarke, B.~D., Burke, C.~J., et al.\ 2013, \apjs, 207, 35

\bibitem[Petigura \etal(2013)]{petigura}
Petigura, E.~A., Howard, A.~W., \& Marcy, G.~W.\ 2013,
Proceedings of the National Academy of Science, 110, 19273

\bibitem[Rappaport \etal(2014)]{psf}
Rappaport, S., Barclay, T., DeVore, J., \etal\ 2014, \apj, 784, 40

\end{thebibliography}

\end{document}
