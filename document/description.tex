% this file is part of the Smart Card project.

\documentclass[letterpaper,12pt,preprint]{hack_aastex}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\usepackage{epsfig}
\usepackage{graphicx}
\newcommand{\sectionname}{Section}
\setlength{\headheight}{2ex}
\setlength{\headsep}{3ex}
\input{hogg_nasa}
\newcommand{\kplr}{\package{kplr}}
\newcommand{\Untrendy}{\package{Untrendy}}
\newcommand{\Turnstile}{\package{IronHorse}}
\newcommand{\Bart}{\package{Bart}}
\newcommand{\emcee}{\package{emcee}}
\newcommand{\TheCreator}{\package{TheCreator}}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\bvec}[1]{\mathbf{#1}}
\pagestyle{myheadings}
\markright{\textsf{\small Hogg \& Foreman-Mackey / probabilistic modeling of %
Kepler data}}

\usepackage{listings}
\lstset{%
    language=Python,
    basicstyle=\footnotesize\ttfamily,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    breaklines=false,
    breakatwhitespace=true,
    identifierstyle=\ttfamily,
    keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
    commentstyle=\color[rgb]{0.4,0.4,0.4},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\begin{document}

\section{Why re-analyze \Kepler\ data?}

The \Kepler\ team is one of the most successful collaborations in the
history of astronomy, and the \Kepler\ data are by no means trivial to
understand and use.  Why would even a plucky twosome consider
competing with this extremely capable group?  The answer is that this
is \emph{not} a proposal to \emph{compete} with the \Kepler\ team.
This is a proposal to enhance the capabilities of the \Kepler\ mission
and give new tools to the entire community to increase the scientific
return from the \Kepler\ data (and many related and future data sets).

The first reason that it makes sense to re-consider the \Kepler\ data
from the ground up is the simple point that it is valuable to have
multiple eyes on the problem.  We bring different prejudices,
different intuitions, and different expertise than exists on the
\Kepler\ team.  In addition, we can analyze the \Kepler\ data in a
more open-ended way since we do not bear the responsibility of
delivering results according to existing schedules and requirements.
That said, we very much hope to create methods and code that make the
\Kepler\ team products more valuable and easier to deliver.

The second reason it makes sense for us to take an independent look at
the \Kepler\ data is that we will bring a strong probabilistic,
causal, generative model approach to everything we do.  In each
component of this project, we are trying to write down a probability
for the data given the parameters, and in each component we are trying
to have the relationship of the parameters to the predictions obey
deep ideas we have about the physical or causal processes that
generate the data.  For certain kinds of (endearing) fanatics---or
perhaps more correctly, under certain kinds of restrictive
assumptions---these probabilistic approaches to data analysis are
guaranteed to succeed over more heuristic methods.  We have god on our
side, in this sense.  But because we are free from project
requirements, we can take a more principled approach to the data
analysis and see if that delivers better results.  If it does (because
we are proposing to generate public, open-source, easy-to-use code)
\emph{everybody wins.}

Finally, the most important reason to bring new people and new ideas
to the \Kepler\ data is that the \Kepler\ data are so damned
important.  The vast majority of known exoplanets are
\Kepler\ discoveries (CITE), and each \Kepler\ discovery brings with
it so much important information about each planetary system (CITE).
The data set has included many multiple-planet systems, and supported
preliminary studies of population statistics, such as multiplicity and
inclination distributions (CITE).  These studies have been made
possible in part by the scale and in part by the simplicity of the
\Kepler\ experiment; it has created a statistically useable sample.
Although \Kepler\ is just the first step of \NASA's exoplanet journey,
it is the best data set in it's class right now, and will retain
unique capabilities for many years to come.  Furthermore, many future
data sets (for example that from \TESS;\ CITE), produce data that are
\Kepler-like in many ways.  \textbf{If we can take the most valuable
  data set in exoplanetary astrophysics and make it substantially more
  valuable with an ADAP-scale project, we can deliver to
  \NASA\ outstanding science per dollar.}

Even if you are convinced that the data need to be re-analyzed, why
should they be reanalyzed by this team?  Although the proposers do not
have a long history in exoplanet science, we do have some experience,
breaking ground in the area of hierarchical modeling of exoplanet
populations (CITE Hogg), and telescope light-field modeling for
exoplanet direct detection and spectroscopy (CITE Oppenheimer).  Much
more importantly, the proposal team has enormous experience in two
areas that are crucial if we are going to squeeze more information out
of the \Kepler\ data stream: We have great experience with calibration
and sensitive statistical analysis of enormous astrophysical data
sests (CITE some eg papers), and we are among the world leaders in
probabilistic modeling of astrophysical data sets (CITE some eg
papers).

On the inference side of this,
one example is that Co-I Foreman-Mackey is the lead developer of \emcee\
package \citep{emcee} for fast, self-tuning Markov-Chain Monte Carlo sampling.
This is by far the fastest-growing MCMC package in the astrophysics community
 (at time of writing it is getting about seven citations per month in the
refereed literature).
On the calibration side of this,
one example is that Co-I Hogg was responsible (along with collaborators) for
the precise self-calibration of the \observatory{Sloan Digital Sky Survey}
imaging data \citep{ubercal}, which in turn led (almost directly)
to discoveries such as ultra-faint Milky Way companion Willman I
 \citep{wil1} and many other substructures \citep[for example,][]{wil2, field, gd1} and the detection of the baryon acoustic feature
in the large-scale structure \citep{bao}.  These two
strengths---calibration and probabilistic modeling---are not unique to
the proposing team, but their combination is unusual and puts us in a
very strong position to deliver extremely valuable new methods to the
\Kepler\ team and larger community.

\section{Tools for easy access to \Kepler\ data}

The official source of Kepler data is the
\observatory{Mikulski Archive for Space Telescopes} (\MAST).
\MAST\ also includes complete catalogs of stellar parameters, \Kepler\
``Object of Interest'' (KOI) parameters, and the measured parameters of
confirmed planets in the \Kepler\ field.
Another catalog of exoplanet parameters is the
\observatory{NASA Exoplanet Archive}%
\footnote{\url{http://exoplanetarchive.ipac.caltech.edu/}}.
The \EA\ hosts a complete mirror of the \MAST\ \Kepler\ tables but
also includes results from other exoplanet studies.
The standard method for accessing data from these catalogs is through a Web
form%
\footnote{\url{http://archive.stsci.edu/kepler/data\_search/search.php}}%
\footnote{\url{http://exoplanetarchive.ipac.caltech.edu/applications/ExoTables%
/upload.html}}
but both services also offer programmatic access via an Application
Programming Interface (API).
An API is required technoogy for automated data analysis;
it is also generally useful to have easy access to data from a standard
scientific programming environment because it lets an interested user apply
standard or standardized analysis tools to new datasets easily.

In general, all programming languages that are commonly used for scientific
analyses (C, C++, Java, Python, R, IDL) have
libraries designed to interact with web services using HTTP requests so it is
possible to access the \Kepler\ data from any language.
That said, HTTP tools tend to be very general;
for any particular \Kepler\ query that a user wants to make,
a complex URL must be constructed ``by hand'' (hard-coded or expert-system
generated), and then the structured text returned by the Web server must be
parsed with another equally hard-coded custom set of tools.
Individual hand-construction of a useful API from HTTP requests can therefore
be time consuming and error prone.
More importantly, it represents repeated work that will be done (and probably
already \emph{has} been done) many times over in the community.
One consequence is duplication of effort; another is that many who would
benefit strongly from an API nonetheless resort to using the HTML web form
and downloading and importing the data piecemeal by hand.
This can be a significant obstacle for the development of automated data
analyses.

Maybe comment on JSON.\

\paragraph{\kplr\ interface:}
We propose to build a Python library to provide easy data access directly
from the Python programming environment.
Python is a good choice for this for several reasons:
Many astronomers have already adopted Python as the main
programming language (CITE dotastro?) in their work, and many more are
converting to Python.
There are many powerful astronomy-specific tools that are becoming part of
the standard scientific stack---especially \package{AstroPy}
 (\url{http://www.astropy.org/}).
Also, Python has many tools---such as \package{iPython}
 (\url{http://ipython.org/}) and the \package{iPython} notebook
 (\url{http://ipython.org/notebook.html})---for interactive data exploration.
Python capabilities are also good skills for graduate students to learn
because they are equally valuable in academia and in non-academic
environments (dot-coms, finance).

Our proposed interface to the \Kepler\ data should satisfy at least the
following requirements:
The proposed interface should satisfy at least the following requirements:
\begin{itemize}
\item The interface should be general enough that any request that could
possibly be made from the web form should be possible.
\item Common tasks (like searching for a particular planet or the top 10
largest KOIs, for example) should be easy.
\item The results should be automatically parsed into a Python object with the
correct types so that it can be easily integrated with existing tools.
\item The results syntax should be consistent for different data types.
\item The code should be extremely easy to install; there should be no
barrier to entry.
\end{itemize}
These requirements are designed to maximize adoption within the Python
exoplaneteer community.
Our experience with the \emcee\ Markov-Chain Monte Carlo sampling package,
and the brand we have developed with that package and other projects,
suggests that we can get very high ``market penetration'' if we meet these
requirements.

Below is an example of the proposed syntax for the module.
This code could be run from the interactive Python prompt or from within a
script.
\begin{lstlisting}
import kplr
client = kplr.API()

# Get the listing for the confirmed planet Kepler-10 b
kepler10b = client.planet("10b")
print(kepler10b)
# <kplr.Planet("Kepler-10 b")>

print(kepler10b.period)
# 0.8374903

# Loop over available datasets and download the light curves from MAST.
for d in kepler10b.data:
    d.fetch()
    # ...
    # Analyse the dataset using d.time, d.sapflux, etc.
    # Plot using matplotlib or whatever.

# Find the 10 largest KOIs.
kois = client.kois(sort=("planet_radius", -1), limit=10)
\end{lstlisting}

The development of this tool has started on \project{GitHub}%
\footnote{\url{https://github.com/dfm/kplr}},
along with documentation and example code.
A pre-release version is already available under the liberal MIT open source
license.

The code installs trivially on computers BY WHAT METHOD DFM?\ GIVE EXAMPLES
HERE;\ IT IS PART OF OUR BRAND.\

\paragraph{Proposal:}
\textbf{We propose to finish building the \kplr\ API code and its documentation.
We propose to keep it released and maintained for easy installation and use.
We propose to upgrade \kplr\ continuously as the \MAST\ and \EA\ data contents and
HTTP-request and returned-data formats evolve,
and to include new data sources as they emerge.}
In particular%
---and importantly for \Bart\ and \TheCreator\ (below)---%
we propose to extend \kplr\ to access and interoperate with
emerging archives of radial velocity data,
which provide such important complementary information to that coming from
the \Kepler\ satellite.

By the end of the project,
we expect the \kplr\ API to be a mature and widely-used interface,
for all of the reasons given above
 (notably the customer-oriented requirements and the widespread use of Python
in the community).
It makes sense to explore methods for long-term stewardship of the code.
\textbf{We propose to explore (at the end of this project) with the \MAST\ and \EA\ teams the possibility of
transfer of the \kplr\ code and documentation ownership.}

\section{De-trending}

For complicated reasons related to crowded-field photometry, flat-fielding,
spacecraft temperature issues, stellar abberation, parallax, proper motion,
and intrinsic stellar variability, a typical \Kepler\ lightcurve shows
variability at the sub-percent level even when there is no transit.
For context, the transits of greatest interest induce brightness dips at the
$10^{-4}$ to $10^{-5}$ level.
Most of the non-transit effects lead to smooth light-curve variations, but in addition
there are also discontinuities at the \Kepler\ quarter boundaries (when the
spacecraft attitude is re-set) and (more challenging) poorly understood Sudden Pixel
Sensitivity Dropouts (SPSDs), which lead to essentially discontinuous jumps.
The principled thing for an exoplanet searcher to do is to \emph{simultaneously}
fit all of these ``trend'' and SPSD effects in a highly parameterized model,
along with all possible exoplanet signals.
However, because these effects are very hard to model interpretably, and since
all these sources of variability are essentially \emph{nuisances} to the
exoplaneteer, almost the entire \Kepler\ community performs heuristic, robust
continuum-estimating procedures to the data.
The continuum estimate is divided out to ``de-trend'' the lightcurve.
In what follows, we will propose a highly effective and somewhat principled
approach to the problem of de-trending, and a set of ideas about improving it.
As with all components of this proposal, our solution involves delivering to the
community open-source code and documentation.

\paragraph{Standard practice:}
The \Kepler\ pipeline produces calibrated lightcurves, and then partially
detrended ``PDC'' outputs that have much of the trends removed.
However, For most precision experiments, the light curves corrected using PDC
are not sufficiently de-trended.
It is standard practice (DFM CITE Dressing, for example) to normalize fluxes by a
running windowed median with a width of a few days (this is a free parameter).
This method is surprisingly effective; it shows that most effects---with the
exception of the SPSDs---take place on multi-day time-scales.
Using the median is extremely robust to outliers; if the median window is wide enough in time,
this procedure doesn't significantly affect any (shorter time-scale) transit signal.

The running windowed median procedure runs into serious trouble when it encounters a SPSD or any
other sharp discontinuity.
The implicit background model must be smooth even across discontinuities; it
will introduce significant structured residuals at SPSDs.
The most responsible procedures in current practice discard the data
near the SPSDs but no literature source is clear on how these points are identified
in practice.
(One further comment is that any investigator interested in long-timescale stellar variability
should not use anything as simplistic as a windowed median.)

A more sophisticated method is the ``Bayesian'' MAP-PDC method (DFM CITE THIS).
It uses co-trending with surrounding stars to remove systematic trends that
are correlated between nearby sources; that is, it capitalizes on spatial
continuity of the spacecraft-induced variability of sources.
It also has some capability to capture and model SPSDs, although this is not
a published aspect of the method (at time of writing).
The MAP-PDC will apparently be included in the ``next'' official release of the
\Kepler\ pipeline but it is not yet available to us for analysis and comparison.
Because the MAP-PDC explicitly tries to model spacecraft-induced effects, it
has the advantage that it is expected to preserve (that is, not model and
remove) as much of the true intrinsic stellar variations (as, say, a running
median does).
Of course this advantage is also a disadvantage for many exoplanet hunters:
Any method that preserves stellar variability places a burden on the exoplanet
scientist to simultaneously model the exoplanet transits and the stellar
variability.
Since most kinds of variable stars (even well-behaved RR Lyrae and the like)
don't have good probabilistic generative models, many exoplaneteers are better
off with methods that remove all stellar variability and leave a lightcurve
that can be modeled by exoplanet transit alone.

% DFM:  WHERE TO PUT THIS CLAIM
%      claim in the literature (with reference to a non-existent
%      Kolodziejczak 2012 paper) that PDC in the Kepler pipeline 8.0 will deal
%      with this but it's not included as part of the standard de-trending.

\paragraph{\Untrendy:}
Our algorithm and implementation---\Untrendy---takes a
different approach.
Using only a single quarter of data as input, the algorithm attempts to
remove all systematic variations---spacecraft, calibration, photometric, and
intrinsic stellar---without affecting the strength or shape of any existing
transit signal.
It also properly captures and accounts for SPSDs.
Initial experiments have produced extremely promising results even when
reproducing the lowest signal-to-noise discoveries in the Kepler catalog
(see \figurename~\ref{fig:untrendy}).

The key features of \Untrendy are as follows:
\begin{itemize}
\item The algorithm is very scalable and local relative to the MAP-PDC
  algorithm.
  In its sensitivity to the data, it is similar to a more robust version of the
  windowed median.
  It obtains locality by using a spline basis for the trends; it obtains
  locality with iteratively reweighted least squares (IRLS; HOGG CITATION).
\item The algorithm automatically finds discontinuities---SPSDs---and inserts
  them into the model as change-points.
  It performs this magic by projecting the residuals away from a smooth fit
  onto a sliding discontinuity basis ``eigenfunction''.
\item The algorithm is agnostic about the \emph{origin} of variability; it
  aggressively tries to model the combined effect of all lightcurve variability
  on multi-day time-scales.
  The goal is production of a clean ``transit-only'' de-trended lightcurve.
\end{itemize}

The proposed version~0.1 algorithm involves an iterative procedure with two distinct
steps.
The first step fits a cubic spline model (with one knot every few days; this
probably works best for denser sampling than the median filter model) using
iteratively re-weighted least-squares (IRLS;\ Hogg: some sort of citation?) to
the \Kepler\ median normalized aperture photometry.
Discontinuities are detected using a matched filter on the residuals
from the model fit in the previous step.
Add break-point knots in at the detected discontinuity locations and return to the first IRLS
step; rinse and repeat.  A demo of the version~0.1 method is shown in
\figurename~\ref{fig:untrendy}.%
\ssfigure{figures/untrend_data.pdf}{%
The long cadence observations of the star Kepler-10.
There are two confirmed exoplanets in this system.
\emph{Top:} the raw aperture photometry from the Kepler data reduction
pipeline.
For plotting purposes, each quarter was normalized by the median flux in the
quarter and then 1 was subtracted to show the variation around the median.
\emph{Middle:} the PDC corrected fluxes.
Again, the fluxes were normalized by the median and then shifted to zero
median.
Note the change of units on the y-axis; the PDC procedure clearly makes good
progress towards removing the main systematic trends in the data.
\emph{Bottom:} the light curve after untrending.
The units on the y-axis are the same as the middle panel.
The results are way better\ldots WAY.%
\label{fig:untrendy}}

IRLS is a good choice for this purpose because it is extremely robust to
outliers, but without excluding them from consideration entirely; that is, it
obtains all the robustness value of sigma-clipping or medianing, without
rendering outliers completely uninformative in the fit.
Of great value in early rinse--repeat rounds (when the SPSDs have not all be identified),
the IRLS optimization is also be insensitive to the outliers caused by the
discontinuities until the model is given more flexibility (in the form of
added knots) at these positions.

The choice of a cubic spline model is
justified---and preferable to a windowed median model---for several reasons:
Firstly, it is significantly simpler (in general) that the median or PDC models.
The number of free parameters in the model is only the total number of knots
in the spline (the number of free parameters in a running median is sometimes
thought of as being ``zero'' but it is in fact very large by any reasonable
accounting; it might be as large as the number of data points in some sense).
Second, the spline model can have variable complexity (or flexibility) at
different points---a fact that we are taking advantage of to correct for
SPSDs.
Third, spline models can be evaluated extremely efficiently; a model
like the one that we propose here is not significantly more computationally
complex than a median filter model.
Fourth, spline models are inherently continuous; the running median is
inherently discontinuous (it interpolates badly); to see this consider what
happens as outlier pixels enter and leave the running window.

\paragraph{Enhanced goals:}
While the version~0.1 version of \Untrendy\ described above is a realistic and
quickly obtainable first step towards better modeling and detection of
exoplanets in the Kepler data stream, it leaves the probabilistically
righteous among us a little unsatisfied.
The cubic spline model suggested above is only a very rough approximation to
the physical (causal) model that we have of the data and by applying this
point estimated model we introduce significantly correlated noise and don't
correctly propagate the uncertainty introduced by the modeling procedure.
An exiting prospect for improving this procedure would be to model the data
using a flexible non-parametric probabilistic model like a Gaussian Process
 (CITE STUFF).
Applying a Gaussian process to a dataset as large as a Kepler light curve is
an interesting problem in computer science but since most of the power in the
systematics is at short time lag, it would be possible to take advantage of
the sparsity of the problem for efficiency (Iain Murray?).

Another interesting idea would be to---as in the MAP-PDC algorithm---take
advantage of correlations between the systematic variations of nearby sources.
In particular, a qualitative study of the some Kepler targets has shown that
the locations of discontinuities tend to be the same for sources that are
nearby on the detector.

One remarkable aspect of the spacecraft-induced lightcurve variability is 
a remarkable repeatability for some stars from year to year; with the
first quarters looking all very similar, and the second, and so on.
This indicates that a good causal model for the variability could probably
spearate spacecraft thermal effects from other effects, and possibly model
them separately, possibly even using multi-quarter information simultaneously.

\paragraph{Proposal:}
\textbf{We propose to release a implementation of version~0.1 of \Untrendy\ as a standalone C
library with Python bindings under a liberal open source license (MIT).}
This algorithm could be easily incorporated into any analysis where a running
median is currently in use.
It would also be a complimentary addition to the standard Kepler data
product along side the PDC and MAP-PDC results.
\textbf{We propose to follow with more advanced releases as we explore,
understand, and succeed in building more principled causal models of some
of the more complicated effects.}

\section{Fast, approximate hypothesis testing}

DFM:\  Explain what is currently done and how.  With citations.

Hogg:\  Explain what \Turnstile\ will do in its first release.

DFM:\  And demo figure.

Hogg:\  Explain what future releases of \Turnstile\ might do.


\section{Probabilistic parameter estimation}

Main features of \Bart:
\begin{itemize}
\item Flexible, non-parametric limb-darkening profile.
\item Fast standalone Fortran library.
\item User-friendly Python bindings with expressive model-building syntax.
\item Efficient sampling using \emcee\ \citep{emcee}.
\end{itemize}

\paragraph{Standard methods for parameter estimation}
The exoplanet community has long understood and been open to data analysis
using rigorous probabilistic inference \citep[][for example]{ford}.
All recent methods of planetary parameter estimation use MCMC (or similar) to
propagate the uncertainties \citep[][to name only a few]{barclay,dressing,%
kepler5,kepler4}.
Despite the fact that everyone uses the same techniques, there is no standard
implementation of exoplanet model building and analysis.
In fact, few of the published analysis papers are accompanied be source code.
There are, of course, a few exceptions.
In particular, \project{EXOFAST} \citep{exofast}---and by extension
\project{TAP}/\project{autoKep} \citep{autokep}---provides a set of IDL
routines for simultaneously modeling light curves and radial velocity
observations using a method called Differential Evolution MCMC.%
Since neither of these projects include a license, they can't legally be used
by other research groups but it seems like the community ignores this fact and
hopes for the best.
Even so, there are numerous improvements that could be made on the existing
tools.
In particular, many astronomers are adopting Python as their main programming
language instead of IDL.\
Also, it's generally a good idea to have a \emph{community supported} code
base instead of a single developer model.


\citet{mandel}

\paragraph{A flexible, non-parametric limb darkening model}
As mentioned above, most analyses of transiting exoplanets rely on the
analytic limb darkening expressions of \citet{mandel}.
These results are extremely restrictive on the form of limb darkening profile
that can be used and the results are still relatively expensive to compute
because they rely heavily on special functions whose values must be computed
numerically.
More recently, \citet{crazyass1} and \citet{crazyass2} have proposed
alternative methods for computing the light curve of a planet transiting a
star with a more general limb darkening profile.
The algorithm proposed by \citet{crazyass1} requires the summation of many
high-order special functions.
DFM:\ do they make claims about efficiency?
The second method \citep{crazyass2} involves a numerical integration for
all but the simplest profiles.

\Bart\ will take a brute force approach.
Since the solution of the overlapping area of occulting disks is extremely
inexpensive to compute, it seems like a good idea to integrate the light curve
shape numerically.
Following the notation of \citet{mandel}, the fraction of the stellar flux
that is occulted by a transiting planet is given---in the general case---by
the area integral
\begin{eqnarray}
\lambda & = & \int_{\bvec{A}_\mathrm{occ}} I(\bvec{r}) \dd^2 \bvec{A}
\end{eqnarray}
where $I(\bvec{r})$ is the (properly normalized) limb darkening profile and
$\bvec{A}_\mathrm{occ}$ is the occulted area.
Assuming that $I(\bvec{r}) \equiv I(r)$ is radially symmetric, this integral
simplifies to
\begin{eqnarray}
\lambda & = & \int_{\bvec{A}_\mathrm{occ}} I(r) \, r \dd r \dd \theta \quad.
\end{eqnarray}
Finally, if we write $I(r)$ as a piecewise constant function with values $I_n$
with breakpoints at arbitrary radii $r_n$, the occulted flux is simply
\begin{eqnarray}
\lambda & = & \sum_n w_n \, I_n
\end{eqnarray}
where the $w_n$ are just differences of non-limb darkened solutions.
Preliminary experiments show that arbitrary models can be computed to
extremely high precision for the same computational cost as the standard
\citet{mandel} models but with significantly more flexibility.

\textit{Our proposal is to release an optimized Fortran or C library for
computing light curves of arbitrary limb darkening models.}


\paragraph{Model building \& parameter estimation}
The current authors are specifically interested in the probabilistic modeling
of observations of transiting exoplanets.
The algorithm described in the previous section provides an efficient and
flexible method of computing a \textit{likelihood function}.
A likelihood function is a parameterization of the probability of observing a
set of data given a set of model parameters and a model of the observational
noise.
As a first step, it is probably reasonable to assume that the uncertainties in
the \Kepler\ data are Gaussian and independent.
The independence assumption is certainly not true because of correlations
introduced by the \Untrendy\ algorithm but there are various ways of dealing
with this using more flexible noise models or wavelet methods (DFM:\ cite some
Winn paper).

Some of the more recent analyses of Kepler data have relied on Markov chain
Monte Carlo (MCMC) to perform correct probabilistic inference in the face of
noisy data.
We have substantial expertise in the domain of MCMC methods \citep{emcee} (HOW
ABOUT SOME OTHER CITATIONS TOO:\ Hou etc.) and our open source MCMC
implementation has already been used by several groups in the field.
Building on our experience releasing and supporting \emcee, we propose to
build and release an open-source tool for modeling transit data built on the
model described above and harnessing the power of the \emcee\ sampler.
While the finished product will depend on all of the tools discussed earlier
in this proposal, development has begun---on \project{GitHub} at
\url{https://github.com/dfm/bart}---on an experimental pre-release version of
the software.

The basic \Bart\ work flow will take (as input) any or all of the following
\begin{itemize}
\item light curves (short and/or long cadence and potentially from other
instruments),
\item radial velocity measurements,
\item astronometric constraints, and
\item parameterized models of the prior probabilities on the model parameters.
\end{itemize}
The results will be in the form of samples from the posterior probability
distribution for the parameters given the ensemble of data.
These samples can then be used to estimate the correctly marginalized
constraints on the values of the planetary and stellar parameters or compute
the marginalized likelihood of the hierarchical parameters describing the
prior distributions.
This second use is necessary for \TheCreator\ project described in the next
section.


\paragraph{Enhanced goals:}
False positive detection using LD discrepancies.


\section{Hierarchical population inference}

pgm intro

DFM:\  Explain what is currently done, perhaps concentrating on Tremaine
paper and competitors.  Also we should touch on theory here.

Hogg:\  What will \TheCreator\ do?

\section{Side projects}

GALEX project

false-positive detection and characterization

variable-star science

\section{Management plan}

This project is the PhD dissertation project of Co-I Foreman-Mackey;
in this sense Foreman-Mackey is the scientific lead of the project,
making decisions about scientific priorities and the ordering of
experiments and code enhancements.  PI Hogg is the faculty advisor on
this dissertation; in this sense Hogg is the management lead of the
project, ensuring that deadlines get met, papers get written, and code
gets properly documented and released.  The budget also includes a
small amount of support for an additional part-time graduate student
and postdoc.  These are budgeted to support short-term scentific
collaborations that make use of the software or improve it.  That is,
some of the sub-parts of the project can be executed by new graduate
students or postdocs at NYU.\

The project is budgeted for three years of project lifetime.  An
approximate schedule is the following

\paragraph{first year:}
Complete and release version~1.0 of \kplr\ API along with complete
documentation.
Release version~0.1
of \Untrendy, \Turnstile, and \Bart, executing the first stages of
approximation identified above, plus complete documentation.  Submit short
papers describing these codes, comparing them with standard community
practice, and showing first results.
Execute \Untrendy\ and \Turnstile\ on the entire KIC;\
publication of all candidate transiting exoplanets, especially those
previously unknown.  Execute \Bart\ on the best or most interesting of
those companions, with an eye to exoplanet characterization and
false-positive rejection.

\paragraph{second year:}
Submit papers describing the results of the \Untrendy, \Turnstile, and
\Bart\ from the first year.
Complete and release version~0.1 of \TheCreator, executing the simplest
meaningful graphical model, as described above, along with documentation.
Execute \Bart\ and \TheCreator\ on the complete set
of exoplanet candidates geenerated in the first year.  Submit paper
describing those results.
With feedback from this full ``closed loop'', re-prioritize enhancements to
\Untrendy\ and \Turnstile; ad  features to \Bart.
Complete and release Version~0.2 code for these packages, along with complete
documentation.
If the archived \Kepler\ data have changed or new relevant data sources have
appeared, upgrade and release version~1.1 of \kplr.
Start side projects on stellar variability, eclipsing binaries, transit-timing
variations, or limb-darkening as appropriate.

\paragraph{third year:}
Following up best hints in results from \TheCreator\ in the second
year, complexify or adjust the graphical model, or launch multiple
models for competition.
Re-run everything on the entire KIC to update the population analysis to
incorporate all improvements.
Submit papers describing improved software and new results.
Complete side projects and submit papers.
Make final decisions about final code versions, final code adjustments,
documentation, and hosting.
Again, if the archived \Kepler\ data have changed or new relevant data sources
have appeared, upgrade and release version~1.2 of \kplr.
Explore possible hand-off for sustainable maintainance of \kplr\ with the NASA archive teams.
Release Version~1.0 for all four of \Untrendy, \Turnstile, \Bart, and
\TheCreator, along with documentation and also (possibly very short) papers
on arXiv or ASCL.

\clearpage
\begin{thebibliography}{}\raggedright%

\bibitem[Barclay \etal(2013)]{barclay}
Barclay, T., Burke, C.~J., Howell, S.~B., \etal\ 2013, \apj, \textbf{768}, 101

\bibitem[Belokurov et al.(2006)]{field}
Belokurov, V., Zucker, D.~B., Evans, N.~W., et al.\ 2006, \apjl, 642, L137 

\bibitem[Borucki \etal(2010)]{kepler4}
Borucki, W.~J., Koch, D.~G., Brown, T.~M., \etal\ 2010, \apjl,
\textbf{713}, L126

\bibitem[Dressing \& Charbonneau(2013)]{dressing}
Dressing, C.~D., \& Charbonneau, D.\ 2013, \apj, \textbf{767}, 95

\bibitem[Eastman et al.(2013)]{exofast}
Eastman, J., Gaudi, B.~S., \& Agol, E.\ 2013, \pasp, \textbf{125}, 83

\bibitem[Eisenstein et al.(2005)]{bao}
Eisenstein, D.~J., Zehavi, I., Hogg, D.~W., et al.\ 2005, \apj, 633, 560

\bibitem[Ford(2005)]{ford}
Ford, E.~B.\ 2005, \aj, \textbf{129}, 1706

\bibitem[Foreman-Mackey \etal(2012)]{emcee}
Foreman-Mackey, D., Hogg, D.~W., Lang, D., \& Goodman, J.\ 2013,
\pasp, \textbf{125}, 306

\bibitem[Gazak et al.(2012)]{autokep}
Gazak, J.~Z., Johnson, J.~A., Tonry, J., \etal\ 2012, Advances in Astronomy

\bibitem[Gim{\'e}nez(2006)]{crazyass1}
Gim{\'e}nez, A.\ 2006, \aap, \textbf{450}, 1231

\bibitem[Grillmair \& Dionatos(2006)]{gd1}
Grillmair, C.~J., \& Dionatos, O.\ 2006, \apjl, 643, L17 

\bibitem[Koch \etal(2010)]{kepler5}
Koch, D.~G., Borucki, W.~J., Rowe, J.~F., et al.\ 2010, \apjl,
\textbf{713}, L131

\bibitem[Kjurkchieva \etal(2013)]{crazyass2}
Kjurkchieva, D., Dimitrov, D., Vladev, A., \& Yotov, V.\ 2013, \mnras,
\textbf{1123}

\bibitem[Mandel \& Agol(2002)]{mandel}
Mandel, K., \& Agol, E.\ 2002, \apjl, \textbf{580}, L171

\bibitem[Padmanabhan et al.(2008)]{ubercal}
Padmanabhan, N., Schlegel, D.~J., Finkbeiner, D.~P., et al.\ 2008, \apj, 674, 1217 

\bibitem[Willman et al.(2005)]{wil1}
Willman, B., Blanton, M.~R., West, A.~A., et al.\ 2005, \aj, 129, 2692

\bibitem[Willman et al.(2005)]{wil2}
Willman, B., Dalcanton, J.~J., Martinez-Delgado, D., et al.\ 2005, \apjl, 626, L85
\end{thebibliography}

\end{document}
