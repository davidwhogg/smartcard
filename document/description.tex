% this file is part of the Smart Card project.

% to-do
% -----
% - DFM: ADD content where necessary; you have room to add.
% - DFM: PUT IN all citations where you find CITE.
% - DFM: REVISE anything written that is unclear or doesn't flow.
% - DFM: GIVE to Carole-Anne for submission.

\documentclass[letterpaper,12pt,preprint]{hack_aastex}

\usepackage{amsmath}
\usepackage{color}
\usepackage[pagebackref=false]{hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.25}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\newcommand{\hurl}[1]{{\scriptsize\url{#1}}}

\usepackage{epsfig}
\usepackage{graphicx}
\newcommand{\sectionname}{Section}
\setlength{\headheight}{2ex}
\setlength{\headsep}{2ex}
\input{hogg_nasa}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\bvec}[1]{{\ensuremath{{\boldsymbol{#1}}}}}
\newcommand{\transpose}[1]{{#1}^{\!\mathsf{T}}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\pagestyle{myheadings}
\markright{\textsf{\footnotesize Hogg \& Foreman-Mackey / %
                   End-to-end probabilistic modeling of Kepler data}}

\usepackage{listings}
\lstset{%
    language=Python,
    basicstyle=\scriptsize\ttfamily,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    breaklines=false,
    breakatwhitespace=true,
    identifierstyle=\ttfamily,
    keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
    commentstyle=\color[rgb]{0.4,0.4,0.4},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\newcommand{\documentname}{\textsl{Proposal}}

\begin{document}

\section{Introduction?}

By discovering thousands of transiting exoplanets, the NASA \Kepler\ Mission has
transformed the study of extra-Solar planets (exoplanets).
In addition to system discoveries, \Kepler\ has enabled
extremely precise measurements,
in which we can see planet--planet interactions (CITE), direct emission from
hot planets (CITE), and even relativistic effects (CITE).

Most importantly from the perspective of this \documentname,
\Kepler\ data have revealed large populations of very small planets,
Earth radius and smaller (CITE).
We now know that most or all of these small planets are almost certainly rocky
(CITE).
Some of these rocky planets even orbit in the putative ``habitable zones'' of
their parent stars, as determined by expected average insolation
(we don't yet know anything about small-planet atmospheres).
In \figurename~\ref{fig:planet-properties} we show the distribution of known
planets in radius ratio (planet-to-star) and orbital period, with the location
of Earth shown.
This is promising for the study of Earth analogs!

\ssfigure{figures/planet_properties.pdf}{0.60}{%
The properties of confirmed and candidate exoplanets from \Kepler.
Data from NASA Exoplanet Archive.
The candidates near the location of Earth (labeled by grey lines) are orbiting
lower-mass (cooler, fainter) stars and are therefore not true Earth analogs,
in terms of size, period, and temperature (or insolation).
\label{fig:planet-properties}}

That said, no \emph{true} Earth analog is yet known.
The known rocky, plausibly habitable exoplanets have all been found around
stars lower mass (and cooler) than the Sun.
They are easier to find around low-mass stars, of course,
because the radius ratio is more favorable, and the habitable zone orbits
correspond to shorter periods.
It also appears to be the case that the abundance of rocky planets around
low-mass stars is higher than that around G-type stars (CITE).

That said, the \Kepler\ data \emph{are} good enough to detect true Earth
analogs.
There are thousands of G-type stars in the \Kepler\ dataset
that are bright enough (in a photon-counting sense) that an Earth-sized
transiting planet on a year-ish period orbit ought to be detectable at good
confidence.
In a project underway now, we are checking whether it is likely---given transit
probabilities under random system inclinations and extrapolations of the
exoplanet population to Earth properties---that the dataset includes a
transiting Earth analog among these thousands of targets.
Some studies say yes (CITE Petigura).
Either way, it would be a crime against all of astronomy if the \Kepler\ data
set is not searched for Earth analogs with tools capable of saturating the
photon-statistics bounds on discovery and inference.

Here we propose to build these tools.
We propose four tools, each capable of increasing the sensitivity and
statistical power of the \Kepler\ mission.
The tools will enable the most sensitive search for Earth analogs ever
conducted.
They will also have wide applicability to other missions, projects, and
objectives.
This is a project that the NASA ADAP program was born to support.

Why is finding an Earth analog so hard?
Why not just loop over periods and phases and start chi-squared fitting?
Fundamentally, the reason is that the \Kepler\ Mission is so very very good:
Never before has photometry been performed at this level of precision (part
in $10^{5}$ or better in terms of raw photon precision);
sure enough, at this unprecedented precision, unprecedented astronomical
issues arise.

For example, even though the satellite points to much better than a hundredth
of a pixel, the tiny variations in pointing cause tiny fractional differences
in the amounts of light hitting neighboring pixels in the vicinity of any
\Kepler\ target.
This variation projected onto any unmodeled flat-field residual or sub-pixel
flat-field variation leads to variations in total counts in the objects.
Similar things happen for temperature-induced optical distortion variations,
and focus changes.
These problems are currently not well modeled by the \Kepler\ official pipelines
and present challenges for every \Kepler\ user working at high precision.

In addition, it turns out that at this level of precision, the great majority
of stars---even the great majority of Sun-like stars---are photometrically
variable in stochastic ways.
That is, there is ``intrinsic'' noise coming from the stars themselves,
over and above photon shot noise.

For context for what follows:
The \Kepler\ mission operates in an extremely simple mode that is designed to
make it insensitive to residual calibration issues in its optics and detector.
For example, it spends three-month periods with its pointing locked down to
much better than pixel precision.
This permits precise photometry without full understanding of the flat-field
or PSF.
Exposures are typically 30~min;
for each target star, not all of the pixels are downloaded every exposure.
Instead, to preserve bandwidth, only a small pixel patch is downloaded for each
star.
Finally, within the downloaded pixel patch, the official \Kepler\ photometry
is based on a straight sum of just a few pixels more-or-less centered on the
star.
As we hope to explain in this \documentname, there is low-hanging fruit to be
picked to make \Kepler's light curves more precise.

Everything in this \documentname\ is about \Kepler, but of course everything we
propose will be released open-source (as is everything done in our group).
That means that everything we propose here that is useful for \Kepler\ will
also be useful for the \project{K2} extended mission, that was just (2014 May 15)
approved for funding in the NASA Response to the 2014 Senior Review.

\section{A data-driven model of the \Kepler\ pixels}

Despite the impressive precision, \Kepler's photometry is plagued by
substantial ``systematics'' due to instrumental effects (pointing shifts,
temperature variations, \etc) and real astrophysical signals (stellar
variability, transiting exoplanets, \etc).
It is of significant scientific interest to separate these two types of
signals and much progress has been made towards removing instrumental
systematics while robustly retaining the astrophysical effects (CITE PDC, ARC,
\etc).
These algorithms are all based on a fundamental \emph{causal} argument: the
signals that are common across nearby targets must be due to instrumental
variations because there can be no causal connection between the astrophysical
objects.
The idea is simple but a lot of the work that goes into implementing these
models involves combating over-fitting.

We propose to implement a method based on the same argument that models the
instrumental effects \emph{at the pixel level} (TPF level)
instead of in the photometry.
This method makes a prediction for the variability caused by the instrument in
a specific pixel at a specific time by using the pixel time series of similar
nearby (but causally disconnected) targets at different times.

We'll model the flux due to systematic variations in pixel $k$ of target
$n$ at time $t$ as
\begin{eqnarray}
f_{nk}(t) &=& \bvec{c}_{nk}^\mathrm{T}\cdot\bvec{f}_{\sim n}(t)
              + \epsilon_{nk}(t)
\end{eqnarray}
where $\bvec{f}_{\sim n}$ is the vector of some $K$ pixels around nearby
targets (not including $n$), $\bvec{c}_{nk}$ is a vector of linear weights,
and $\epsilon_{nk}$ represents the stochastic pixel noise.
There are many choices that must be made to evaluate this model but the main
ones are: (a) the number of pixels $K$ should be used in $\bvec{f}_{\sim n}$,
and (b) how the weights $\bvec{c}_{nk}$ are chosen.

In principle, as $K$ gets large, the model will tend to over-fit the data.
Motivated by the standard techniques in the machine learning literature, we
advocate for a very large value of $K$ but to use other techniques to regularize
and avoid over-fitting.
In particular, we choose to fit the coefficients $\bvec{c}$ using the light
curve in this pixel but at \emph{different times} $t^\prime$ where
$|t-t^\prime| > \Delta$.
That is, we use a ``train-and-test'' framework to control model freedom.
We also use some Gaussian regularization that draws weakly controlled parameters
towards zero.
\figurename~\ref{fig:plm} shows an example, in comparison with standard \Kepler
data products.

\ssfigure{figures/kepler-20-plm.pdf}{0.60}{%
A comparison of photometric methods applied to quarter 9 of target Kepler-20,
a variable exoplanet host with 5 known transiting exoplanets.
This star is known to exhibit significant stellar variability and we chose it
as an example because the presearch data conditioning algorithm clearly
over-fits the signals.
With sensible choices for the hyperparameters of our data-driven model, the
resulting light curve retains more of the interesting astrophysical signals
and does not affect the transit depths.
\emph{Top:} The \Kepler\ simple aperture photometry (black points) with the data-driven
prediction for the instrumental effects.
\emph{Middle:} Our data-driven photometric light curve, which removes instrumental effects but preserves stellar variability and transit depths.
\emph{Bottom:} The \Kepler\ presearch data conditioning light curve, which over-fits the data, removing real variability.
\label{fig:plm}}

The \PLM\ produces a prediction for every pixel under the assumption
(effectively) of a non-variable sky.
How do we plan to use this prediction?
There are several options, including subtracting it away or dividing it out.
Our plan is to photometer the data according to the \Kepler\ SAP prescription,
and then also photometer the \PLM\ prediction of the data, and present the
ratio of these as the \PLM\ photometry.
In detail, this is what \figurename~\ref{fig:plm} shows.
(We have shown a variable star in \figurename~\ref{fig:plm}, to emphasize that
the \PLM\ preserves intrinsic variability and delivers very different results
from the official \Kepler\ ``detrending'' PDC pipeline.)

Here we propose to build the \PLM\ and work through the choices of
train-and-test subsampling, regularization, and $K$ to find high-quality
options for the \Kepler\ data.
We propose to run it on a significant fraction of the
\Kepler\ data (at least all the Sun-like stars).
We also propose to release the code in a very useable form (for example, ``input
a \Kepler\ target and get back the \PLM\ photometry'') so that it can be used
by anyone in the community, including the \project{K2} team.
We propose to write up at least one paper in the refereed literature describing
the \PLM.
In what follows, we will propose further to use the \PLM\ model and photometry
as inputs to our photometric estimator codes and variability modeling codes.

\section{Optimized aperture photometry}

The astronomical community tends to think in terms of stellar flux estimators
(or photometric estimators) that are weighted linear sums of pixels.
If you have an image of an isolated star and you know it's position in the
image, the point-spread function, the approximate brightness of the star, and
all the parameters of the pixel-level noise model, it is possible to obtain a
photometric estimate of the star of this form (weighted linear sum of pixels)
that is optimized for signal-to-noise.
The estimator is a kind of ``matched filter''.
This result has been known for a long time (CITE THINGS).

Below (when we talk about \kpsf) we are going to consider more radical
photometric estimators.
But even within the restriction to estimators based on weighted linear sums,
there are advances to be made.
In the case of \Kepler\ the opportunities come from the fact that the data
are taken in an extremely uniform, homogeneous mode, but at the same time not
\emph{perfectly} homogeneous.
That is, the pointing of \Kepler\ is stable only at the hundredth-of-a-pixel
level (or maybe thousandth; we don't precisely know), and it undergoes
temperature and focus variations as it adjusts for attitude changes required
for data downlink operations.

If we think of the ``jitter'' or small unknown displacements of the angular
position of the satellite and the optics in focus or temperature changes as a
kind of ``noise'', then even though the individual pixel read-outs are
independent, the variance in the pixel space obtains non-trivial covariances:
The pointing and PSF variance maps onto a correlated pixel variance.

Ideally, we would model these using knowledge of the \Kepler\ PSF and
pointing jitter.
Unfortunately, neither of these things is precisely known at present (but see
our \kpsf\ proposal below).
They aren't known, but there are a lot of data!
It turns out that we can determine the ``s/c jitter noise'' (really a pointing,
focus, and temperature noise) empirically;
we can measure the empirical mean and variance of the pixels in the patch of
imaging centered on each relevant star.

If you think of a read-out pixel patch from exposure $n$ taken at time
$t_n$ as being a little data ``vector'' $d_n$ (19 pixels or a 19-vector
in the example shown in \figurename~\ref{fig:owl}), then we can obtain an
empirical mean $\mu$ (19-vector) and an empirical covariance $C$ ($19\times 19$
symmetric, positive definite matrix).
If the mean and covariance are well estimated---that is, if they
represent the true mean jittered PSF and the true covariance of the
pixel values around that mean---and if the covariance is dominated by
telescope and detector noise sources---that is, not intrinsic
variability of the star---then these estimated quantities $\mu$ and
$C$ can be used to construct optimal weights for linear photometric
estimators on the data.
These estimators are truly novel:
They are signal-to-noise-optimized estimators in the presence of noise coming
from spacecraft positional jitter and other effects that correlate pixel noise.
However, they are also simple generalizations of the ``matched filter''
estimators used to perform optimal photometry and optimal spectroscopic
extraction (CITE).
We show an example in \figurename~\ref{fig:owl}.
We propose here to build a software package called ``the \OWL'' that constructs
these estimators.
We also propose to explore the decision space that we allude to below.

\ssfiguretwo{figures/kic_03335426_05_images_dopw.png}%
            {figures/kic_03335426_05_diff_photometry.png}{0.60}{%
An example of the \OWL\ photometry acting on one quarter of \Kepler\ data
on one target star.
In the top panel, one of the optimized soft photometric apertures is shown,
in comparison with the standard \Kepler\ SAP aperture.
In the bottom panel the photometric light curves are shown.
The curves marked ``DOWL'', ``DOPW'', and ``DTSA'' are different versions of the
\OWL\ method.
\OWL\ removes the trends completely from the data,
revealing a low-level rotation-period variability (presumably from star spots).
The removal of the trends comes at some cost in pixel-to-pixel variability,
because the soft aperture created by the \OWL\ is forced to be orthogonal to
substantial positional jitter noise.
These results are very preliminary, but they demonstrate feasibility.
\label{fig:owl}}

It turns out that there are many photometric estimators constructable as
optimizations of some estimate of signal-to-noise; for this reason we call
the outputs of the \OWL\ ``optimized'' rather than ``optimal'' estimators;
no estimator is optimal for all purposes.
One class of decisions is related to how we estimate the empirical mean $\mu$
and covariance $C$:
Do we clip outliers?
Do we consider variations on all time-scales?
Do we somehow include or exclude intrinsic stellar variability?

Another class of decisions is related to regularization or control of the
photometric aperture.
In the results shown in \figurename~\ref{fig:owl} we permitted an arbitrarly
shaped and weighted aperture in pixel space.
It would be possible to restrict the aperture to have a round shape, or a
top-hat shape, or a Gaussian, or whatever.
For each such choice---and each method of estimating $\mu$ and $C$---there is
an optimal aperture for performing photometry.
All these choices need to be explored.

The output of the \OWL\ is a new photometric aperture, which can be used to
create photometry that is perfectly analogous to the existing \Kepler\
photometry.
The existing photometry is obtained through an optimized aperture, but that
aperture is optimized using a theoretical point-spread function, ideas about
field crowding, and assumptions of minimal s/c jitter.
The \OWL\ produces photometry that is either higher in signal-to-noise or else
detrended from s/c movements, or (in excellent cases) both.
That is, the \OWL\ photometry, built as it is with a data-driven optimization,
is expected to be better than any existing \Kepler\ product,
at least for isolated stars that are low in intrinsic variability
(more on this below).

One key idea of this \documentname\ is that the \OWL\ plays very well the \PLM.
That is, as the \PLM\ does a better and better job of calibrating the
instrument (and the \PLM\ effectively calibrates out some parts of the s/c
jitter noise), the empirical covariance seen by the \OWL\ will either be
smaller in amplitude or else informed by the \PLM\ model directly;
indeed the \PLM\ is effectively a model of all the s/c jitter contributions to
the empirical noise covariance.

We propose to fully develop the \OWL\ software and explore the choices available
in estimation of $\mu$ and $C$ and also in aperture regularization or
constraint.
We propose to run the tuned \OWL\ on a large number of \Kepler\ targets
(certainly including all the Sun-like stars).
We also propose to release all the code in a simple-to-use form so that it
can be adopted by any \Kepler\ user and also the \project{K2} team if they
want it.
We propose to write up the \OWL\ in at least one paper in the scientific
literature.

\section{Flexible, non-parametric modeling of light curve variations}

The \PLM\ and the \OWL\ are designed to find and eradicate s/c jitter
contributions to the light curve noise or morphology.
But we emphasized in the introduction that essentially all stars vary
stochastically at the precision of the \Kepler\ photometry.
This stochastic variability is effectively a ``noise'' from the point of view
of exoplanet discovery and characterization.
In principle the stochastic variability might be interesting in its own right,
but for us it is noise or a nuisance.

In the \Kepler\ exoplanet community, the standard approach to this problem is
to either filter the data (for example, subtract out or divide out a running
boxcar median of the data; CITE)
or else fit flexible models (such as polynomials)
to small patches of the data around the transits (CITE).
These approaches have been fairly successful, but they suffer from a number
of issues when it comes to measurement:
If the filtering or fitting happens \emph{prior} to transit search or inference,
then the uncertainty induced by the filtering or modeling is not propagated
correctly into the transit results.
If the fitting happens simultaneously with the transit inference, uncertainty
can be propagated but there are issues of under-fitting or inappropriate model
choice that are difficult to address.

We propose to build a code---\George---that addresses these issues using the
mature technology of Gaussian Processes (CITE).
One way of thinking about a Gaussian Process is that it provides a prior PDF
over an infinite dimensional \emph{function space} of all possible stochastic
stellar variability histories.
Another way of thinking about a Gaussian Process is that it is a swap-in
replacement for a $\chi^2$-based likelihood function;
it is a model of the variance and covariance structure of the stochastic
variation.

In traditional fitting, the likelihood looks like
\begin{eqnarray}
-2\,\ln L(\theta) &=& \sum_n \frac{[d_n - m_n(\theta)]^2}{\sigma_n^2} + \sum_n \ln (2\pi\,\sigma_n^2)
\quad ,
\end{eqnarray}
where $L$ is the likelihood,
$d_n$ is the $n$th data point,
$m_n(\theta)$ is the expected value of that datum given parameters $\theta$,
$\sigma_n^2$ is the noise variance for datum $n$.
In a Gaussian Process, the likelihood looks like
\begin{eqnarray}
-2\,\ln L(\theta) &=& \transpose{[d - m(\theta)]}\,\inverse{\Sigma}\,[d - m(\theta)] + \ln (2\pi\,\det{\Sigma})
\\
\Sigma_{nn'} &\equiv& \sigma_n^2\,\delta_{nn'} + K(t_n - t_n')
\quad ,
\end{eqnarray}
where $d$ is the column vector made of all the $d_n$,
$m(\theta)$ is the column vector made of all the $m_n(\theta)$,
and $\Sigma$ is a huge positive-definite covariance matrix,
built from a covariance function $K$ of time differences.
The Gaussian Process reduces to traditional $\chi^2$ in the limit that $K$
vanishes.
When $K$ is made non-trivial, the Gaussian Process can capture correlated
noise of the kind that is generated by real variable stars.
One example of a Gaussian Process fit to a real \Kepler\ target is shown in
\figurename~\ref{fig:george}.

\ssfigure{figures/kic-10593626-synth.pdf}{0.60}{%
\Kepler\ short-cadence data for KIC 10593626 with a synthetic injected transit
(points) and 50 posterior samples (lines) from an MCMC analysis with nine
parameters (7 physical and 2 hyperparameters for the noise model).
\label{fig:george}}

There are many choices for the kernel function, but the choices we make are
parameterized usually by an overall variance and a correlation time.
As the variance parameter is increased, the overall stellar variability is
increased; as the correlation time is increased, the variability gets
``smoother''.
One great thing about Gaussian Processes is that these ``hyperparameters''
(variance and correlation time) can be inferred or optimized just like
any other model parameters.
The worst thing about Gaussian Processes is that they involve performing
non-trivial operations on enormous matrices;
we are involved in a project to make these linear algebra operations much
faster than they have been made previously (CITE siva).

We propose to build \George,
a general-purpose code for performing exoplanet transit
fitting with a Gaussian Process likelihood function.
This code will build on work we have done to perform very fast evaluation
of limb-darkened transit light curves with planets on arbitrary Keplerian
orbits.
We also propose to explore methods for setting hyperparameters,
including likelihood optimization, cross-validation, and full inference.

If we can set the hyperparameters well, the beautiful thing about \George\ is
that is provides a simple drop-in replacement for $\chi^2$ wherever this kind
of fitting is being done.
This makes \George\ of enormous applicability in many areas that go way beyond
exoplanets.

\section{The \Kepler\ focal plane}

In a perfect world where we could infer a full physical, time-dependent model
of \Kepler's focal plane, the \PLM\ and \OWL\ procedures would be obviated.
In practice, this is a very difficult problem and data-driven models might
continue to be the best method for measuring ultra-precise relative photometry
for isolated stars.

That being said, there are some places where a robust model of the focal
plane---including the flat field and the point spread function, for
example---is necessary.
There are, for example, some blended sources in the \Kepler\ field-of-view and
de-blending the light curves requires a full likelihood model (\citealt{psf}).
Studies of full-pipeline recovery reliability (\citealt{inject}) involve
injecting transit signals at the pixel level.
This operation is only as reliable as our model of the focal plane.
If the next-generation \Kepler\ mission is approved, the pointing of the
telescope will be much less stable than during the original mission.
In this case, a model of the focal plane will be useful for measuring
photometry of the stars as they drift.

In order to improve these procedures and (possibly) generate more precise
light curves, we propose to build a full probabilistic generative model for
the flat field and point spread function of the \Kepler\ detector.
This model will include a spatially and temporally varying PSF,
a temporally varying DC or sky background,
a sub-pixel flat-field (more below),
and the position and brightness as a function of time (light curve) for every
star.
Given all these parameters or parameterized functions, we can write down a
full-data likelihood or probability for the entire \Kepler\ pixel-level (TPF)
data set given the parameters.
This is the object on which fitting and inference would be based.

We have only tried tiny steps towards this model, but they are very promising.
In a white paper we wrote, we looked at a few of the issues (CITE),
and we have done some experiments subsequently.
It looks like we can obtain an extremely good PSF model,
principally because the data are so extremely high in signal-to-noise.

As for the flat-field, it is hard to get good constraints on this,
because the s/c jitter is so small, there isn't much movement between the
stars and the focal plane.
However, we think that to properly model the variations in the data as
the s/c drifts, it is necessary to consider the heterogeneity in the individual
pixel sub-pixel sensitivity functions.
This requires making a sensitivity map or flat-field model that has more
parameters than there are pixels!
That isn't a problem in principle, because there are so many exposures
and such high signal-to-noise in each.

Although we can see in general that this modeling is possible, we don't know
precisely what effects will dominate nor which we should model first, given
our priorities.
For example, at the sensitivity level we care about, stellar aberration will
probably matter, and perhaps even parallax and proper motion.
Prioritization of these different model aspects will be made by starting with
the simplest possible model (flat-field, background, PSF, and stellar light
curves) and then fitting and residual investigation.
The residuals will show morphologies that point to the biggest unmodeled
effects.
This all may sound ambitious!
But the PI (along with Dustin Lang at CMU) has been building models of this
kind in a framework and
codebase---\project{The Tractor}---that is under development for modeling
\project{SDSS}, \project{WISE}, and \project{CFHTLS} data.

In principle, the light curves returned by such a model will be the best that
can possibly be made; there are frequentist bounds and Bayesian principles
that say so.
Those guarantees only apply when the model is good, so whether they really
will beat the \PLM\ and \OWL\ for isolated sources is not yet clear.
However, for crowded fields, overlapping sources, or auxilliary operations
like signal injection, a physical model is a necessary component.

We propose to build causal, physical, image-modeling
software---tentatively called ``\kpsf''---for likelihood fitting and Bayesian
inference of all s/c parameters, detector sensitivity maps, and stellar
positions and light curves.
We propose to run it and explore its residuals to constrain development to
the dominant terms in the overall noise budget.
We propose to run the developed versions on at least the \Kepler\ G-type
stars and release all outputs, which include a great deal of valuable
engineering data about the satellite along with the light curves.
We propose to write up this software in a paper or series of papers,
and release all the code open-source at all stages of development.
Just as with all the other components of this project, we expect this to be
very valuable for re-analysis also of \project{K2} data.

\section{Management plan and schedule}

In the first year, we plan to complete the \OWL\ and \George\ projects.
These projects are already started and the latter comprises part of the PhD
dissertation work of Co-I Foreman-Mackey.
PI Hogg will lead the \OWL\ work and the Co-I will lead the \George\ project.
At least draft manuscripts will be written for both projects, they will
be submitted in the first year if at all possible.
The Co-I will begin a search for Earth analogs in the G-type stars in the
\Kepler\ data using the \George\ likelihood and the \OWL\ photometric outputs.
This search will effectively form a full-scale, end-to-end functional test of
both pieces of software.
The PI will open a search for a postdoctoral scholar (postdoc).

In the second year, Co-I Foreman-Mackey will graduate and the postdoc will
begin work on the \PLM.
The goal will be to get at least a draft of the \PLM\ paper ready by the end
of year two.
The PI and Co-I will shepherd through the publication process the \OWL\ and
\George\ papers.
The part-time graduate student employee will work with the PI to
re-run another Earth analog search, but this time
as a full-scale, end-to-end functional test of the \PLM\ data (but of course
also using the \OWL\ and \George).

In the third year, the PI and postdoc will work together on \kpsf, with all
heavy lifting done by the postdoc.
The PI and postdoc will shepherd the \PLM\ paper through the publication
process and also write and submit the \kpsf\ paper for publication.
The PI will work with the part-time graduate student to write up the previous
year's search and also to start a third full search, this time to
fully test the \kpsf\ model.

\textbf{All code will be released under the MIT open-source license.}
This is a simple license that permits reuse, modification, and re-release of
the code.
We also keep all code available on the Web and licensed at every stage of
the project, so in fact the community can intervene, comment, or build on our
work at any stage in the development process.
We will work with the community and also the \project{K2} team to aid in
adoption of the code in scientific projects of all kinds.

\clearpage
\begin{thebibliography}{}\raggedright%

\bibitem[Christiansen \etal(2013)]{inject}
Christiansen, J.~L., Clarke, B.~D., Burke, C.~J., et al.\ 2013, \apjs, 207, 35

\bibitem[Petigura \etal(2013)]{petigura}
Petigura, E.~A., Howard, A.~W., \& Marcy, G.~W.\ 2013,
Proceedings of the National Academy of Science, 110, 19273

\bibitem[Rappaport \etal(2014)]{psf}
Rappaport, S., Barclay, T., DeVore, J., \etal\ 2014, \apj, 784, 40

\end{thebibliography}

\end{document}
