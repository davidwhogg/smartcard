% this file is part of the Smart Card project.

\documentclass[letterpaper,12pt,preprint]{hack_aastex}

\usepackage{color}
\usepackage[pagebackref=false]{hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.25}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\newcommand{\hurl}[1]{{\scriptsize\url{#1}}}

\usepackage{epsfig}
\usepackage{graphicx}
\newcommand{\sectionname}{Section}
\setlength{\headheight}{2ex}
\setlength{\headsep}{3ex}
\input{hogg_nasa}
\newcommand{\kplr}{\package{kplr}}
\newcommand{\Untrendy}{\package{Untrendy}}
\newcommand{\Bart}{\package{Bart}}
\newcommand{\emcee}{\package{emcee}}
\newcommand{\TheCreator}{\package{TheCreator}}
\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\bvec}[1]{\mathbf{#1}}
\pagestyle{myheadings}
\markright{\textsf{\small Hogg \& Foreman-Mackey / %
                   End-to-end probabilistic modeling of Kepler data}}

\usepackage{listings}
\lstset{%
    language=Python,
    basicstyle=\scriptsize\ttfamily,
    showspaces=false,
    showstringspaces=false,
    tabsize=2,
    breaklines=false,
    breakatwhitespace=true,
    identifierstyle=\ttfamily,
    keywordstyle=\bfseries\color[rgb]{0.133,0.545,0.133},
    commentstyle=\color[rgb]{0.4,0.4,0.4},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\begin{document}

\section{Why re-analyze \Kepler\ data?}

The \Kepler\ team is one of the most successful collaborations in the
history of astronomy, and the \Kepler\ data are by no means trivial to
understand and use.  Why would even a plucky twosome consider
competing with this extremely capable group?  The answer is that this
is \emph{not} a proposal to \emph{compete} with the \Kepler\ team.
This is a proposal to enhance the capabilities of the \Kepler\ mission
and give new tools to the entire community to increase the scientific
return from the \Kepler\ data (and many related and future data sets).

The first reason that it makes sense to re-consider the \Kepler\ data
from the ground up is that it is valuable to have
multiple eyes on the problem.  We bring different prejudices,
intuitions, and expertise than exists on the
\Kepler\ team.  In addition, we can analyze the \Kepler\ data in a
more open-ended way; we do not bear the responsibility of
mission management.
That said, we very much hope to create methods and code that make the
\Kepler\ team products more valuable and easier to deliver.

The second reason it makes sense for us to take an independent look
is that we bring a strong probabilistic,
causal, generative model approach to everything we do.  In each
component of this project, we are trying to write down a probability
for the data given the parameters, and to structure this
according to
the ideas we have about the physical or causal processes that
generate the data.  For certain kinds of fanatics---or
perhaps more correctly, under certain kinds of restrictive
assumptions---these probabilistic approaches to data analysis are
guaranteed to succeed over more heuristic methods.
If we can take a principled approach, and also
generate public, open-source, easy-to-use code,
\emph{everybody wins.}

Finally, the most important reason to bring new people and new ideas
to the \Kepler\ data is that the \Kepler\ data are so damned
important.  The vast majority of known exoplanets are
\Kepler\ discoveries, and each discovery brings with
it so much important information about the planetary system.
The data set has included many multiple-planet systems, and supported
preliminary studies of population statistics.  These studies have been made
possible in part by the scale and in part by the simplicity of the
\Kepler\ experiment; it has created a statistically useable sample.
Although \Kepler\ is just the first step of \NASA's exoplanet journey,
it is the best data set in it's class right now, and will retain
a unique role for many years.  Furthermore, future
data sets (for example that from \TESS) produce data that are
\Kepler-like in many ways.  \emph{If we can take the most valuable
  data set in exoplanetary astrophysics and make it substantially more
  valuable with an ADAP-scale project, we can deliver to
  \NASA\ outstanding science per dollar.}

Even if you are convinced that the data need to be re-analyzed, why
should they be reanalyzed by \emph{this team?}  Although we do not
have a long history in exoplanet science, we do have some experience,
breaking ground in the area of hierarchical modeling of exoplanet
populations \citep{hogg-ecc}, and telescope light-field modeling for
exoplanet direct detection and spectroscopy \citep{opp}.  Much
more importantly, we have experience in two
areas that are crucial for squeezing information out
of the \Kepler\ data stream: We have great experience with calibration
and sensitive statistical analysis of enormous astrophysical data
sests and we are among the world leaders in
probabilistic modeling of astrophysical data sets.

On the inference side of this,
Co-I Foreman-Mackey is the lead developer of \emcee\
package \citep{emcee} for fast, self-tuning Markov-Chain Monte Carlo sampling.
This is by far the fastest-growing MCMC package in the astrophysics community
 (at time of writing it is getting about seven citations per month in the
refereed literature).
On the calibration side of this,
Co-I Hogg was responsible (along with collaborators) for
the precise self-calibration of the \observatory{Sloan Digital Sky Survey}
imaging data \citep{ubercal}, which in turn led (almost directly)
to discoveries such as ultra-faint Milky Way companion Willman I
 \citep{wil1} and many other substructures \citep[for example,][]{wil2, field, gd1} and the detection of the baryon acoustic feature
in the large-scale structure \citep{bao}.  These
strengths---calibration and probabilistic modeling---are not unique to
the proposing team, but their combination is unusual and puts us in a
very strong position to deliver extremely valuable new methods to the
\Kepler\ team and larger community.

\section{Tools for easy access to \Kepler\ data}

The official source of \Kepler\ data is the
\observatory{Mikulski Archive for Space Telescopes}
(\MAST; \hurl{http://archive.stsci.edu/kepler/data\_search/search.php}).
\MAST\ also includes complete catalogs of stellar parameters, \Kepler\
``Object of Interest'' (KOI) parameters, and the measured parameters of
confirmed planets in the \Kepler\ field.
Another source is the
\observatory{NASA Exoplanet Archive} (\hurl{http://exoplanetarchive.ipac.caltech.edu/}).
The \EA\ hosts a complete mirror of the \MAST\ \Kepler\ tables but
also includes results from other exoplanet studies.
The standard method for accessing data from these archives is via Web
form,
but both services also offer programmatic access via an Application
Programming Interface (API).
An API is required technoogy for automated data analysis.

In general, all programming languages that are commonly used for scientific
analyses (C, C++, Java, Python, R, IDL) have
libraries designed to interact with web services using HTTP requests so it is
possible to access the \Kepler\ data from any language.
That said, HTTP tools tend to be very general;
for any particular \Kepler\ query that a user wants to make,
a complex URL must be constructed ``by hand'' (hard-coded or
expert-system-generated), and then the structured text returned by the Web server must be
parsed with another equally hard-coded custom tool.
Individual hand-construction of a useful API can therefore
be time consuming and error-prone.
More importantly, it represents repeated work that will be done (and probably
already \emph{has} been done) many times over in the community.
One consequence is duplication of effort; another is that many who would
benefit strongly from an API nonetheless resort to using the Web form
and downloading and importing the data piecemeal by hand.

\paragraph{\kplr\ interface:}
We propose to build a Python library to provide easy data access directly
from the Python programming environment.
Python is a good choice for this for several reasons:
Many astronomers have already adopted Python as the main
programming language in their work, and many more are
converting to Python.
There are many powerful astronomy-specific tools that are becoming part of
the standard scientific stack---especially \package{AstroPy}
 (\hurl{http://www.astropy.org/}).
Also, Python has many tools---such as \package{iPython}
 (\hurl{http://ipython.org/}) and the \package{iPython} notebook---%
for interactive data exploration.
Python is also a good skill for a graduate students to learn
because it is equally valuable in academia and in non-academic
environments (dot-coms, finance).

Our proposed interface to the \Kepler\ data should satisfy at least the
following requirements:
\begin{itemize}
\item The API should be general enough that any request that could
possibly be made from the web form should be possible.
\item Common tasks (like searching for a particular planet or the top 10
brightest KOIs, for example) should be easy.
\item The results should be automatically parsed into a Python object with the
correct types so that it can be easily integrated with existing tools.
\item The results syntax should be consistent for different data types.
\item The code should be extremely easy to install; there should be no
barrier to entry.
\end{itemize}
These requirements are designed to maximize adoption within the Python
exoplaneteer community.
Our experience with the \emcee\ Markov-Chain Monte Carlo sampling package,
and the brand we have developed with that package and other projects,
suggests that we will get very high ``market penetration'' if we meet these
requirements.

Here is an example of the proposed syntax for the module:
\begin{lstlisting}
import kplr
client = kplr.API()

# Get the listing for the confirmed planet \Kepler-10 b
kepler10b = client.planet("10b")
print(kepler10b)
# <kplr.Planet("Kepler-10 b")>

print(kepler10b.period)
# 0.8374903

# Loop over available datasets and download the light curves from MAST.
for d in kepler10b.data:
    d.fetch()
    # ...
    # Analyse the dataset using d.time, d.sapflux, etc.
    # Plot using matplotlib or whatever.

# Find the 10 largest KOIs.
kois = client.kois(sort=("planet_radius", -1), limit=10)
\end{lstlisting}
The development of this tool has started on \project{GitHub}
(\hurl{https://github.com/dfm/kplr}),
along with documentation and example code.
A pre-release version is already available under the liberal MIT open source
license.

The code installs trivially on computers using the standard Python build
systems. The recommended installation method uses \project{pip}
(\hurl{http://www.pip-installer.org/}); the command is simply
\begin{lstlisting}
pip install kplr
\end{lstlisting}

\paragraph{Proposal:}
\emph{We propose to finish building the \kplr\ API code and its documentation.
We propose to keep it released and maintained for easy installation and use.}
We propose to upgrade \kplr\ continuously as the \MAST\ and \EA\ data contents and
HTTP-request and returned-data formats evolve,
and to include new data sources as they emerge.
In particular%
---and importantly for \Bart\ and \TheCreator\ (below)---%
we propose to extend \kplr\ to access and interoperate with
emerging archives of radial velocity data,
which provide such important complementary information to that coming from
the \Kepler\ satellite.

By the end of the project,
we expect the \kplr\ API to be a mature and widely-used interface,
for all of the reasons given above;
it makes sense to explore methods for long-term stewardship of the code.
\emph{We propose to explore (at the end of this project) with the \MAST\ and \EA\ teams the possibility of
transfer of the \kplr\ code and documentation ownership.}


\section{De-trending}

For complicated reasons related to crowded-field photometry, flat-fielding,
spacecraft temperature issues and thrust activity, stellar abberation, parallax, proper motion,
and intrinsic stellar variability, a typical \Kepler\ lightcurve shows
variability at the sub-percent level even when there is no transit.
For context, the transits of greatest interest induce brightness dips at the
$10^{-4}$ to $10^{-5}$ level.
Most of the non-transit effects lead to smooth light-curve variations, but in addition
there are discontinuities at the \Kepler\ quarter boundaries (when the
spacecraft attitude is re-set) and poorly understood Sudden Pixel
Sensitivity Dropouts (SPSDs), which lead to discontinuous jumps.
The principled thing for an exoplanet searcher to do is to \emph{simultaneously}
fit all of these ``trend'' and SPSD effects in a highly parameterized model,
along with all possible exoplanet signals.
However, because these effects are very hard to model interpretably, and since
all these sources of variability are essentially \emph{nuisances} to the
exoplaneteer, almost the entire \Kepler\ community performs heuristic, robust
continuum-estimating procedures to the data.
The continuum estimate is divided out to ``de-trend'' the lightcurve.
In what follows, we will propose a highly effective and somewhat principled
approach to the problem of de-trending, and a set of ideas about improving it.
As with all components of this proposal, our solution involves delivering
open-source code and documentation.

\paragraph{Standard practice:}
The \Kepler\ pipeline produces calibrated lightcurves, and partially
detrended ``PDC'' outputs that have much of the trends removed.
However, For most precision experiments, the PDC-corrected light curves
are not sufficiently de-trended.
It is standard practice to normalize fluxes by a
running windowed median with a width of a few days \citep{dressing} or a
polynomial fit to the out-of-transit flux values \citep[regions chosen by
hand;][]{autokep}.
These methods are surprisingly effective; they show that most effects---with
the exception of the SPSDs---take place on multi-day time-scales.
The median is robust to outliers; if the median window is
wide enough, standard procedures don't significantly affect any
transit signal.

Both of these procedures run into serious trouble however when they encounter a SPSD
or any other sharp discontinuity.
The implicit background model must be smooth even across discontinuities; it
will introduce significant structured residuals at SPSDs.
The most responsible procedures in current practice discard the data
near the SPSDs but no literature source is clear on how these points are
identified in practice.

A more sophisticated method is the ``Bayesian'' MAP-PDC method
\citep{map-pdc1,map-pdc2}.
It capitalizes on spatial
continuity of the spacecraft-induced variability of sources and
has some capability to capture and model SPSDs, although this is not
a published aspect of the method (at time of writing).
The MAP-PDC will apparently be included in the ``next'' official release of the
\Kepler\ pipeline but it is not yet available to us for analysis and comparison.
Because the MAP-PDC explicitly tries to model spacecraft-induced effects, it
has the advantage that it is expected to preserve (that is, not model and
remove) as much of the true intrinsic stellar variations.
Of course this advantage is also a disadvantage for many exoplanet hunters:
Any method that preserves stellar variability places a burden on the exoplanet
scientist to simultaneously model the exoplanet transits and the stellar
variability!
Since most kinds of variable stars (even well-behaved periodic variables)
don't have good probabilistic generative models, many exoplaneteers are better
off with methods that remove all stellar variability and leave a lightcurve
that can be modeled by exoplanet transit alone; that's our approach.

\paragraph{\Untrendy:}
Our algorithm and implementation takes a
different approach.
Using only a single quarter of data as input, the algorithm attempts to
remove all systematic variations---spacecraft, calibration, photometric, and
intrinsic stellar---without affecting the strength or shape of any existing
transit signal.
It also properly captures and accounts for SPSDs.
Initial experiments have produced extremely promising results even when
reproducing the lowest signal-to-noise discoveries in the \Kepler\ catalog
(see \figurename~\ref{fig:untrendy}).

The key features of \Untrendy are as follows:
\begin{itemize}
\item The algorithm is very scalable and local relative to the MAP-PDC
  algorithm.  It has the locality and robustness of the
  windowed median but
  it obtains locality by using a spline basis for the trends.  It obtains
  robustness with iteratively reweighted least squares (IRLS; HOGG CITATION).
\item The algorithm automatically finds discontinuities---SPSDs---and inserts
  them into the model as change-points.
  It performs this magic by projecting the residuals away from a smooth fit
  onto a sliding discontinuity basis ``eigenfunction'' or matched filter.
\item The algorithm is agnostic about the \emph{origin} of variability; it
  aggressively tries to model the combined effect of all lightcurve variability
  on multi-day time-scales.
  The goal is production of a clean ``transit-only'' de-trended lightcurve.
\end{itemize}

The proposed version~0.1 algorithm involves an iterative procedure with two distinct
steps.
The first step fits a cubic spline model (with one knot every few days; this
probably works best for denser sampling than the median filter model) using
iteratively re-weighted least-squares (IRLS;\ Hogg: some sort of citation?) to
the \Kepler\ median normalized aperture photometry.
Discontinuities are detected using a matched filter on the residuals
from the model fit in the previous step.
Add break-point knots in at the detected discontinuity locations and return to the first IRLS
step; rinse and repeat.  A demo of the version~0.1 method is shown in
\figurename~\ref{fig:untrendy}.%
\ssfigure{figures/untrend_data.pdf}{0.60}{%
The long cadence observations of the star \Kepler-10.
There are two confirmed exoplanets in this system.
\emph{Top:} the raw aperture photometry from the \Kepler\ data reduction
pipeline.
For plotting purposes, each quarter was normalized by the median flux in the
quarter and then unity was subtracted to show the variation around the median.
\emph{Middle:} the PDC corrected fluxes.
Again, the fluxes were normalized by the median and then shifted to zero
median.
Note the change of units on the y-axis; the PDC procedure clearly makes good
progress towards removing the main systematic trends in the data.
\emph{Bottom:} the light curve after untrending.
The units on the y-axis are the same as the middle panel.
The results are way better\ldots WAY.%
\label{fig:untrendy}}

IRLS is a good choice for \Untrendy\ because it is extremely robust to
outliers, but does not exclude them from consideration entirely; that is, it
obtains all the robustness value of sigma-clipping or medianing, without
rendering outliers completely uninformative in the fit.
Of great value in early rinse--repeat rounds (when the SPSDs have not all be identified),
the IRLS optimization is also insensitive to the
discontinuities until the model is given more flexibility (in the form of
added knots) at these positions.

The choice of a cubic spline model is
preferable to a windowed median model:
Firstly, it has significantly fewer degrees of freedom than the median or PDC models.
The number of free parameters is just the total number of knots
in the spline (the number of free parameters in a running median is
very large by any
accounting).
Second, the spline model can have variable flexibility at
different points---a fact that we are taking advantage of to correct for
SPSDs.
Third, spline models can be evaluated extremely efficiently; a model
like the one that we propose here is not significantly more computationally
complex than a median filter model.
Fourth, spline models are inherently continuous; the running median is
inherently discontinuous (it interpolates badly); to see this consider what
happens as outlier pixels enter and leave the running window.

\paragraph{Enhanced goals:}
While the version~0.1 version of \Untrendy\ described above is a realistic and
quickly attainable first step towards better modeling and detection of
exoplanets, it leaves the probabilistically
righteous among us a little unsatisfied.
\begin{itemize}
\item
The cubic spline model suggested above is only a very rough approximation to
any physical model of the data; also the point estimation introduces
correlated noise and doesn't
correctly propagate the uncertainty.
A prospect for improving this procedure would be to model the data
using a flexible non-parametric probabilistic model like a Gaussian Process
 (DFM CITE Rasmussen).
Applying a Gaussian process to a dataset as large as a \Kepler\ light curve is
an interesting problem in computer science, but since most of the power in the
systematics is at short time lag, it will be possible to take advantage of
the sparsity of the problem.
\item
It would be good---as in the MAP-PDC
algorithm---to take advantage of correlations between the systematic
variations of nearby sources.
In particular, SPSDs and other
systematics tend to be the same or related for sources that are nearby
on the detector.
\item
There is remarkable repeatability in the spacecraft-induced
variability for some stars from year to year, with the
first quarters looking all very similar and so on.
This indicates that a good physical model for the variability could probably
separate spacecraft thermal effects from other effects, possibly
permit \Untrendy\ to use multi-quarter information simultaneously.
\item
There are known 3-day (thruster) and 370-day (sidereal orbit) periods
in the data; \Untrendy\ should know about these and fit them actively.
\end{itemize}

\paragraph{Proposal:}
\emph{We propose to release a implementation of version~0.1 of \Untrendy\ as a standalone C
library with Python bindings under a liberal open source license (MIT).}
This would be a complimentary addition to the standard \Kepler\ data
product along side the PDC and MAP-PDC results.
\emph{We propose to follow with more advanced releases as we explore,
understand, and succeed in building more principled causal models of some
of the more complicated effects.}

The key idea behind \Untrendy\ is that by removing all non-transit
sources of variability, it makes transits much easier to find.
\emph{We propose to run \Untrendy\ on all \Kepler\ lightcurves (the
  entire KIC) and then search every one for periodic transits.  We
  propose to publish all candidates.}
This search will procede
according to standard \Kepler-community methods such as the ``box
least squares'' \citep{box}, perhaps optimized by us for our particular
hardware (think GPU) architectures.
We will perform this search ``in
the open'', sharing results with the community as we get them, but
then writing up a statistically sound description of the entire
process for those (like us) using the results to seed probabilistic
analyses.
(If any reviewer doubts our willingness to work out in the open, the
full progress of the writing of this proposal, from outline to final
form, with all comments and issues and history, is available at
\hurl{https://github.com/davidwhogg/smartcard}.)


\section{Probabilistic parameter estimation}

The most exciting science to come from \Kepler\ recently has been in
the \emph{characterization} of the exoplanet systems---the understand
of extra-solar planetary systems in terms of planet sizes, distances,
inferred compositions and expected surface temperatures.  These
results depend on \emph{probabilistic inference}.  The \Kepler\ and
larger exoplanet communities have become very sophisticated, making
use of justifiable likelihood functions, reasonable priors, Bayesian
inference, marginalization over nuisance parameters, and MCMC
sampling.  What we are contributing here is \emph{good engineering}
that will make all of these activities easier, higher performing, and
more useful.  We also add some important new capabilities (and, of course, release everything open-source).

\paragraph{Standard practice:}
The exoplanet community has long understood and been open to data analysis
using rigorous probabilistic inference \citep[for example,][]{ford}.
All recent methods of planetary parameter estimation use MCMC (or similar) to
propagate the uncertainties \citep[][to name only a few]{barclay,dressing,%
kepler5,kepler4}.
Despite the fact that a large fraction of exoplaneteers use these kinds of techniques, there is no standard
implementation for exoplanet model-building and analysis.
In fact, few of the published analysis papers are accompanied by openly visible or available source code at all, let alone useful, reuseable tools.
There are, of course, a few exceptions.
In particular, \project{EXOFAST} \citep{exofast}---and by extension
\project{TAP}/\project{autoKep} \citep{autokep}---provides a set of IDL
routines for simultaneously modeling light curves and radial velocity
observations using a method called Differential Evolution MCMC.\@
Since neither of these projects include a license, they can't (in a strict legal sense) be used
by other research groups but it seems like the community (for now) ignores this fact and
hopes for the best.
(One of what the Other Astrophysics Agency might call ``broader impacts'' of this project could be that it will help to propagate thoughtful licensing decisions into the community.)
Even so, there are also improvements to be made.
In particular, many astronomers are adopting Python as their main programming
language, and it is a good idea to have a \emph{community supported} code
base instead of the ascendent single-developer model.

\paragraph{\Bart:}
This component of the proposal is a package of methods and code
designed to analyze \emph{individual} exoplanet systems (stars), with
one or more exoplanets causing transits or radial velocity variations
or both.  It is designed to propagate full likelihood (or posterior
probability) information forward to population synthesis projects.
The requirements of a game-changing exoplanet inference system are the following:
\begin{itemize}
\item It must be fast.  In the case of \Bart, fastness is provided by a standalone Fortran library that computes the transit lightcurves and radial-velocity predictions.
\item It must have user-friendly Python bindings and permit scientifically expressive model-building.  Adoption in the community for \Bart\ will be aided by its built-in flexibility about model parameter specification:  Systems can be parameterized with more observational parameters like times and periods, or more physical parameters like planet mass-densities and orbital angular momenta.
\item It must be flexible about transit appearances, otherwise inferences about planet properties will be constrained with more code-delivered precision than is scientifically warranted.  \Bart\ achieves this goal through inclusion of a non-parametric limb-darkening profile.
\item It must be efficient in sampling; here \Bart\ takes advantage of
  our very successful \emcee\ \citep{emcee} and our continuing
  research into MCMC methods.
\end{itemize}
An example of the action of our prototype \Bart\ code is shown in \figurename~\ref{fig:bart}.
\ssfigure{figures/bart.pdf}{0.70}{%
The results of an experimental \Bart\ analysis on the long cadence light curve
for the planet \Kepler-6b.
The grid of contour plots and histograms show the projections of the MCMC
samples for the physical parameters of the planet (made under some standard prior PDF choices): semi-major axis $a$, radius
$r$, mid-transit epoch $t_0$ (in days), and inclination $i$ (in degrees).
The inset panel in the top right shows the data folded on the median period in
the samples and 12 randomly drawn model light curves overplotted.
\label{fig:bart}}

\paragraph{\Bart\ / flexible, non-parametric limb darkening model:}
As mentioned above, most analyses rely on the
analytic limb darkening expressions of \citet{mandel}.
These results are extremely restrictive on the form of limb darkening profile
that can be used and the results are still relatively expensive to compute
because they rely heavily on special functions.
More recently, \citet{crazyass1} and \citet{crazyass2} have proposed
alternative methods for computing the light curve of a planet transiting a
star with a more general limb darkening profile; these also require
either the summation of many
high-order special functions or else a numerical integration for
all but the simplest profiles.

\Bart\ will take a brute force approach.
Since the solution of the overlapping area of occulting disks is extremely
inexpensive to compute, it seems like a good idea to integrate the light curve
shape numerically.
In the usual way of thinking, the fraction of the stellar flux
that is occulted by a transiting planet is given by an
area integral of a smooth, analytic approximation to limb-darkening function $I(r)$.
The \Bart\ approach is to write $I(r)$ as a piecewise constant function
with breakpoints at arbitrary radii $r_n$.  The occulted flux becomes a
weighted sum of simple geometric areas.
Preliminary experiments show that arbitrary limb-darkening models can be computed to
extremely high precision for the same computational cost as the standard
\citet{mandel} models but with significantly more flexibility.
Priors on the step function can be used to restrict the flexibility as needed.

\paragraph{\Bart\ / Model building and parameter estimation:}
We are specifically interested in the probabilistic modeling
of observations of transiting exoplanets; the limb-darkening work is designed with making such inferences more accurate (although, presumably, a bit less precise).
The algorithm described in the previous section provides an efficient and
flexible method of computing a \textit{likelihood function}.
A likelihood function is a parameterization of the probability of observing a
set of data given a set of model parameters and a model of the observational
noise.
As a first step, it is probably reasonable to assume that the uncertainties in
the \Kepler\ data are Gaussian and independent.
The independence assumption is certainly not true because of correlations
introduced by the \Untrendy\ algorithm but there are various ways of dealing
with this using more flexible noise models or wavelet methods (DFM:\ cite some
Winn paper).

Some of the more recent analyses of \Kepler\ data have relied on Markov chain
Monte Carlo (MCMC) to perform correct probabilistic inference in the face of
noisy data.
We have substantial expertise in the domain of MCMC methods \citep{emcee} (HOW
ABOUT SOME OTHER CITATIONS TOO:\ Hou etc.) and our open source MCMC
implementation has already been used by several groups in the field.
Building on our experience releasing and supporting \emcee, we propose to
build and release an open-source tool for modeling transit data built on the
model described above and harnessing the power of the \emcee\ sampler.
While the finished product will depend on all of the tools discussed earlier
in this proposal, development has begun---on \project{GitHub} at
\hurl{https://github.com/dfm/bart}---on an experimental pre-release version of
the software.

The basic \Bart\ work flow will take (as input) any or all of the following
(1)~light curves (short and/or long cadence and potentially from other
instruments),
(2)~radial velocity measurements,
(3)~astrometric constraints, and
(4)~parameterized models of the prior probabilities on the model parameters.
The results will be in the form of samples from the posterior probability
distribution for the parameters given the ensemble of data.
These samples can then be used to estimate the correctly marginalized
constraints on the values of the planetary and stellar parameters or compute
the marginalized likelihood of the hierarchical parameters describing the
prior distributions.
This second use is necessary for \TheCreator\ project described below.

One key idea for both population modeling and for uptake by the
community is that parameterization of exoplanet systems must be very
flexible and user-settable.  For example, some purists want to model
the systems in terms only of (more-or-less) directly observable
properties, such as the ratio of exoplanet-to-star solid angles and
the orbital periods.  Others want to work in a ``physical
characterization'' mode in which planets have densities (set by
composition) and orbits have conserved quantities (energies and
angular momenta).  Still others work in hybrid modes.  A huge barrier
to code sharing, both on the code-writer and code-user sides, is
created by these decisions.  \emph{\Bart\ takes a completely agnostic
attitude toward parameterization; any user-definable function of
exoplanet properties can be used for setting priors, parameterizing
the likelihood function, MCMC sampling, and inference.}

\paragraph{Enhanced goals:}
Though we have working prototype code (shown in
\figurename~\ref{fig:bart} and available on the web at
\hurl{https://github.com/dfm/bart}), there are many many areas where we
would like to explore extensions and improvements.
\begin{itemize}
\item
A small fraction of \Kepler\ KOIs end up being false positives
generated by (interesting) eclipsing binaries mixed with brigter stars
(DFM CITE SOMETHING).  These can be distinguished from true exoplanets
with follow-up observations (for example, DFM CITE) or detailed
eclipse modeling (CITE BLENDER).  With our flexible limb-darkening
model, we expect the false positives to be limb-darkening outliers;
exploration of this could be very valuable for \Kepler\ follow-up
efforts.
\item
Along the same lines, it is interesting and not difficult to extend
\Bart\ into a full modeling system for eclipsing binaries.  In its
current form, it will treat planets as totally dark.  They aren't, of
course, and are even less dark when the eclipsing body is either a
young planet or not a planet at all.
\item
In prototype form, \Bart\ requires user specification of the number of
exoplanets and reasonable initialization.  We have done work with
Brendon Brewer (DFM CITE) regarding MCMC in problems with variable
complexity (think here: variable numbers of exoplanets).  Some of this
will be necessary in cases where some exoplanets are only marginally
detected or we need to pass off very high-quality information to
hierarchical analyses.
\item
The ESA \Gaia\ mission (HOGG CITE) is about to deliver
thousands---maybe tens of thousands---of exoplanets discovered through
astrometric wobble of the primary star.  The \Gaia\ mission will
require good inference; if \Bart\ can contribute that would please us.
Also, this enhancement is not enormous, because \Bart\ already
computes radial velocity wobbles; the astrometric wobbles are
separated only by an integral (ie, analytic transformation).
Furthermore, the \Gaia\ internal data plan is to release not just
best-fit information about its targets but also astrometric residuals.
Any project that can mine those residuals for new objects or to create
likelihood functions for hierarcical inference will do very well.
Finally, the \Gaia\ footprint will include the \Kepler\ and
\TESS\ surveys, so even from a NASA-partisan perspective this would be
of great value.
\end{itemize}

\paragraph{Proposal:}
\emph{Our proposal is to release a complete, flexible package for
  probabilistic exoplanet characterization.}  This package will
include an optimized Fortran library for computing light curves of
arbitrary limb darkening models, and fast, efficient MCMC sampling in
Python.  The code will be easy to install and easy to use in the
Python idiom.  Because it will be agnostic about parameterization, it
will conform to the needs of many users.

There are many enhancements to make to this basic plan, \emph{We
  propose to make judgements about these and proceed as opportunities
  warrant.}  We are excited about automatic false-positive detection,
introducing more advanced sampling techniques, and preparing \Bart\ to
make use of forthcoming \Gaia\ astrometric data.  However, there are
many other opportunities that might arise as the exoplanet scientific
landscape is evolving rapidly; we propose to be responsive to these
changes and act with good judgement.


\section{Hierarchical population inference}

One of the main science goals of the \Kepler\ mission is to learn about the
\emph{global population} exoplanets and answer questions like ``do planetary
systems tend to form aligned in a disk?'', and ``what fraction of planets live
in multiple planet systems?''
These ``architecture'' studies are (in principle)
excellent examples of \emph{hierarchical inference problem}
 (CITE:\ Hogg ecc, Blei, other?).
While the term ``hierarchical inference'' is not often used in the astronomy
literature, many astrophysical research questions can be posed this way and
techniques from the computer science literature---where hierarchical inference
is an active research problem---can be applied.

The basic structure of a hierarchical analysis involves determining the
structure and parameters of the \emph{prior} probability distribution
functions conditioned on observations of an \emph{ensemble} of independent
systems.
An illustrative way of representing an inference problem like this is a
\emph{probabilistic graphical model} (PGM).
PGMs are a visual representation of the conditional dependencies in a
probability distribution or equivalently the generative procedure for the data
as implied by the assumed model.
An extremely general model for the \Kepler\ inference is problem is shown in
\figurename~\ref{fig:full-pgm}.
\ssfigure{figures/full-pgm.pdf}{0.4}{%
The general probabilistic model describing the \Kepler\ hierarchical inference
problem.
At the top, the parameters of the global population of planet is indicated by
$\Theta_\mathrm{p}$ and the global distribution of planetary systems in
labeled $\Theta_\star$.
The parameters of the planetary systems $\star_n$ and the parameters of the
planets $p_n^m$ are then drawn from these global distributions.
Finally, after applying the survey systematics $S$ (this includes things like
the selection function of the survey and other general systematic effects),
the observations $\mathbf{X}_n$---light curves, radial velocities, etc.---are
noisily drawn by ``observing'' the physical system.
\label{fig:full-pgm}}

The problem of learning about the population of exoplanets can be formulated
in terms of a maximization of the marginalized likelihood function of
$\Theta_p$ conditioned on \emph{all observations of all exoplanets}:
\begin{eqnarray}
p(\{\mathbf{X}_n\}\,|\,\Theta_p) = \int \prod_{n}
p(\{p_n^m\},\,\star_n,\,\mathbf{X}_n\,|\,\Theta_p)
\dd p_n^m \dd\!\star_n\quad.
\end{eqnarray}
The marginalization in this computation is most efficiently performed using
MCMC.
In particular, the results of an analysis using \Bart\ include the information
necessary for accurately estimating the value of the marginalized likelihood.

The principal idea of the hierarchical inference is that the
high-level population parameters (semi-major-axis distribution
parameters, for example) get connected (through probability
distributions) directly to the low-level data (\Kepler\ photometry, in
this case).
This connection can only happen if the \emph{selection
  functions}---for \Kepler\ to observe a star and for a planetary
system to make it through our search filters---are part of the model,
preferably with parameters.
Typically selection functions are determined (point-estimated) using fake
data or data insertions and then used to re-weight the data in population
analyses.
In a better hierarchical analysis, the selection functions are fit and marginalized
out in the inference, constrained by both fake data (and data insertions) and the
real data on which the population inference is being performed.
This procedure properly captures all information about the selection function
available in the superset including the real and fake data, and also properly
propagates uncertainty in the selection function into final inferences.

\paragraph{Standard practice:}
Some important first steps have been taken toward modeling the population
parameters \citep[for example,][]{lissauer,tremaine,fang} with somewhat
contradictory results.
For example, \citet{fang} conclude that the distribution of \Kepler\ planet
candidates strongly implies that systems with multiple planets tend to lie in
relatively thin disks while \citet{tremaine}---using the same
dataset---conclude that there isn't enough information in the \Kepler\ dataset
to constrain the inclination distribution.
The inclusion of results from radial velocity surveys seem to bring the
results into better agreement but the differences are
puzzling.
\citet{fang} model
the data by comparing statistics on simulated populations of exoplanets
 (samples from a uniform prior in the parameters) and \citet{tremaine}
directly model statistics on the individual light curves.
All of the architectural studies to date have relied on the assumption that it
is possible to obtain a ``complete'' sample of exoplanetary systems by making
hard cuts on the observed parameter space.
As a result, these studies are required to be extremely conservative when
selecting data.


\paragraph{\TheCreator:}
We propose to build a general tool---called \TheCreator---for performing
hierarchical inference in the context of exoplanet population modeling.
This will build on and rely heavily on the efficient samplings provided by
\Bart.
With extensive experience in the field of hierarchical inference
\citep{xd,hogg-ecc,stargal}, we are well positioned to undertake this
ambitious analysis using world class probabilistic methods.
The basic work flow for \TheCreator, will take as input a list of light
curves and radial velocities for an ensemble of exoplanetary systems.
The output will be the maximum likelihood non-parametric model for the global
prior functions of various physical parameters.
A simple example of this technique is described in \citet{hogg-ecc} where we
demonstrate a hierarchical method for learning the distribution of planet
eccentricities given a set of synthetic radial velocity observations.

Hierarchical inference requires likelihood tools for the internal parts
of the graphical model.
\Bart\ provides \emph{posterior} samplings, which
are not identical to likelihood functions.
However, we have shown in previous work that we can perform an importance
sampling that permits us to use posterior samplings in the internal parts
of a hierarchical model very efficiently \citep{hogg-ecc}.

We have only just started this project; here are the directions we propose to go:
\begin{itemize}
\item
The absolutely most crucial element of \TheCreator\ is that it models
\emph{the data} not \emph{statistics of the data}.  That is, the prior
PDFs in \TheCreator\ are PDFs for exoplanet system parameters, which
in turn are parameters for the likelihood function in \Bart\ acting on
the data directly.  By integrating \Bart\ samplings in \TheCreator, we
will be the first team in astrophysics to be modeling the whole
exoplanet population, constrained by the lightcurve photometric points
directly.
\item
\TheCreator\ needs to fully confront the problem of sampling in models
of variable complexity (Cite Brewer).  It is both the case that we
need to transmit information about marginal detections and
non-detections, and that we have unspecified model complexity for some
of the non-parametric PDFs in the hierarchical model.  The sampling in
\TheCreator\ is going to have to be able to jump between models with
different numbers of planets in all KOI systems.
\item
Most analyses to date either assume that exoplanets are independently
assigned to stars---that is, without regard to other exoplanets that
might be there---or else make strong, rigid assumptions about
multiplicity and period distributions.  We know (both from dynamics
and from observations) that planets are \emph{not} independent but
rather interact strongly; we need to generate flexible
parameterizations for distrbutions simultaneously over planet number
\emph{and} the properties of each planet.
\item
Related to this, it is possible that advances in understandings of
planetary dynamics or approximations thereto will lead to useful
``dynamical priors'' that can be applied to avoid modeling systems or
families of systems that would not be stable over long time scales.
We already use ``no planet-crossing'' priors in some of our projects,
but these are crude (to say the least).
\item
There is great excitement in the planet-formation community about
population syntheses from first principles, based on star and planet
formation models.  These can only connect to what we intend with
\TheCreator\ if we can perform our hierarchical analyses in terms of
planet compositions and other quantities related to formation and
migration of planets.
\item
Above all, we need to not only gather together all relevant data
(photometric, dynamical, and astrometric); we need to build (within
\Bart) likelihood functions for all of them.
\end{itemize}


\paragraph{Proposal:}
Fundamentally, \emph{we propose to bring full hierarchical
  probabilistic inference to exoplanet data, including the
  \Kepler\ data.}  We plan to build and release a general tool for
such inference.  \emph{We also propose to use this tool to perform
  simple hierarchical analyses in models of increasing complexity.}
Our first inferences will be in extremely simplified models, such as
those in the literature at present, which treat the population as
being determined by a few multiplicity and inclination parameters, but
we intend to get to more realistic models as the project proceeds.  We
also propose to make it an early goal to strongly rule out and learn
from the ``independent-planet'' straw-man model that underpins many
current analyses, though must certainly be wrong.

Once planet-independence is broken, one of the key goals of this
project is to \emph{develop parameterizations for priors over
  exoplanetary systems (``architecture'') that permit planet
  dependence but are still very flexible or non-parametric}.  We
propose to develop these and use them to model the full data set we
have collected for our simpler analyses.


\section{Management plan and schedule}

This project is the PhD dissertation project of Co-I Foreman-Mackey;
in this sense Foreman-Mackey is the scientific lead of the project,
making decisions about scientific priorities and the ordering of
experiments and code enhancements.  PI Hogg is the faculty advisor on
this dissertation; in this sense Hogg is the management lead of the
project, ensuring that deadlines get met, papers get written, and code
gets properly documented and released.  The budget also includes a
small amount of support for an additional part-time 
postdoc.  These are budgeted to support short-term scentific
collaborations that make use of the software or improve it.
The project is budgeted for three years of project lifetime:

\paragraph{first year:}
Complete and release version~1.0 of \kplr\ API along with complete
documentation.
Release version~0.1
of \Untrendy\ and \Bart, executing the first stages of
approximation identified above, plus complete documentation.  Submit short
papers describing these codes, comparing them with standard community
practice, and showing first results.
Execute \Untrendy\ on the entire KIC, followed by transit-searching;\
publication of all candidate transiting exoplanets, especially those
previously unknown.  Execute \Bart\ on the best or most interesting of
those companions, with an eye to exoplanet characterization and
false-positive rejection.

\paragraph{second year:}
Submit papers describing the results of \Untrendy\ and
\Bart\ from the first year.
Complete and release version~0.1 of \TheCreator, executing the simplest
meaningful graphical model, as described above, along with documentation.
Execute \Bart\ and \TheCreator\ on the complete set
of exoplanet candidates geenerated in the first year.  Submit paper
describing those results.
With feedback from this full ``closed loop'', re-prioritize enhancements to
\Untrendy; add features to \Bart.
Complete and release Version~0.2 code for these packages, along with complete
documentation.
If the archived \Kepler\ data have changed or new relevant data sources have
appeared, upgrade and release version~1.1 of \kplr.

\paragraph{third year:}
Following up best hints in results from \TheCreator\ in the second
year, complexify or adjust the graphical model, or launch multiple
models for competition.
Re-run everything on the entire KIC to update the population analysis to
incorporate all improvements.
Submit papers describing improved software and new results.
Make final decisions about final code versions, final code adjustments,
documentation, and hosting.
Again, if the archived \Kepler\ data have changed or new relevant data sources
have appeared, upgrade and release version~1.2 of \kplr.
Explore possible hand-off for sustainable maintainance of \kplr\ with the NASA archive teams.
Release Version~1.0 for all three of \Untrendy, \Bart, and
\TheCreator, along with documentation and also (possibly very short) papers
on arXiv or ASCL.

\clearpage
\begin{thebibliography}{}\raggedright%

\bibitem[Barclay \etal(2013)]{barclay}
Barclay, T., Burke, C.~J., Howell, S.~B., \etal\ 2013, \apj, {768}, 101

\bibitem[Belokurov et al.(2006)]{field}
Belokurov, V., Zucker, D.~B., Evans, N.~W., et al.\ 2006, \apjl, 642, L137

\bibitem[Borucki \etal(2010)]{kepler4}
Borucki, W.~J., Koch, D.~G., Brown, T.~M., \etal\ 2010, \apjl, {713}, L126

\bibitem[Bovy Jo \etal(2011)]{xd}
Bovy Jo, Hogg, D.~W., \& Roweis, S.~T.\ 2011, Annals of Applied Statistics,
5, 1657

\bibitem[Dressing \& Charbonneau(2013)]{dressing}
Dressing, C.~D., \& Charbonneau, D.\ 2013, \apj, {767}, 95

\bibitem[Eastman et al.(2013)]{exofast}
Eastman, J., Gaudi, B.~S., \& Agol, E.\ 2013, \pasp, {125}, 83

\bibitem[Eisenstein et al.(2005)]{bao}
Eisenstein, D.~J., Zehavi, I., Hogg, D.~W., et al.\ 2005, \apj, 633, 560

\bibitem[Fadely \etal(2012)]{stargal}
Fadely, R., Hogg, D.~W., \& Willman, B.\ 2012, \apj, 760, 15

\bibitem[Fang \& Margot(2012)]{fang}
Fang, J., \& Margot, J.-L.\ 2012, \apj, 761, 92

\bibitem[Ford(2005)]{ford}
Ford, E.~B.\ 2005, \aj, {129}, 1706

\bibitem[Foreman-Mackey \etal(2012)]{emcee}
Foreman-Mackey, D., Hogg, D.~W., Lang, D., \& Goodman, J.\ 2013,
\pasp, {125}, 306

\bibitem[Gazak et al.(2012)]{autokep}
Gazak, J.~Z., Johnson, J.~A., Tonry, J., \etal\ 2012, Advances in Astronomy

\bibitem[Gim{\'e}nez(2006)]{crazyass1}
Gim{\'e}nez, A.\ 2006, \aap, {450}, 1231

\bibitem[Grillmair \& Dionatos(2006)]{gd1}
Grillmair, C.~J., \& Dionatos, O.\ 2006, \apjl, 643, L17

\bibitem[Hogg \etal(2010)]{hogg-ecc}
Hogg, D.~W., Myers, A.~D., \& Bovy, J.\ 2010, \apj, 725, 2166

\bibitem[Koch \etal(2010)]{kepler5}
Koch, D.~G., Borucki, W.~J., Rowe, J.~F., et al.\ 2010, \apjl,
713, L131

\bibitem[Kjurkchieva \etal(2013)]{crazyass2}
Kjurkchieva, D., Dimitrov, D., Vladev, A., \& Yotov, V.\ 2013, \mnras,
1123

\bibitem[Kov{\'a}cs \etal(2002)]{box}
Kov{\'a}cs, G., Zucker, S., \& Mazeh, T.\ 2002, \aap, 391, 369

\bibitem[Lissauer \etal(2011)]{lissauer}
Lissauer, J.~J., Ragozzine, D., Fabrycky, D.~C., \etal\ 2011, \apjs, 197, 8

\bibitem[Mandel \& Agol(2002)]{mandel}
Mandel, K., \& Agol, E.\ 2002, \apjl, 580, L171

\bibitem[Oppenheimer \etal(2013)]{opp}
Oppenheimer, B.~R., Baranec, C., Beichman, C., et al.\ 2013, \apj, 768, 24

\bibitem[Padmanabhan et al.(2008)]{ubercal}
Padmanabhan, N., Schlegel, D.~J., Finkbeiner, D.~P., \etal\ 2008, \apj, 674, 1217

\bibitem[Smith \etal(2012)]{map-pdc2}
Smith, J.~C., Stumpe, M.~C., Van Cleve, J.~E., \etal\ 2012, \pasp, 124, 1000

\bibitem[Stumpe \etal(2012)]{map-pdc1}
Stumpe, M.~C., Smith, J.~C., Van Cleve, J.~E., \etal\ 2012, \pasp, 124, 985

\bibitem[Tremaine \& Dong(2012)]{tremaine}
Tremaine, S., \& Dong, S.\ 2012, \aj, 143, 94

\bibitem[Willman et al.(2005)]{wil1}
Willman, B., Blanton, M.~R., West, A.~A., et al.\ 2005, \aj, 129, 2692

\bibitem[Willman et al.(2005)]{wil2}
Willman, B., Dalcanton, J.~J., Martinez-Delgado, D., et al.\ 2005, \apjl, 626, L85
\end{thebibliography}

\end{document}
